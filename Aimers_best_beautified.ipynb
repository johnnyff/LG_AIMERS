{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqWXXYRFuDEy"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnDcv0-7Xrem"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s8ELJIgkXsBG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "# !pip install catboost\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import catboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "# !pip install filterpy\n",
    "import math\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "import itertools\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fuNqIm91XsDz"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(1422) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z2pp9fBhXsM-"
   },
   "outputs": [],
   "source": [
    "base= ''\n",
    "\n",
    "train_df = pd.read_csv(base+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NSyImCAPXsPu"
   },
   "outputs": [],
   "source": [
    "train_x = train_df.filter(regex='X') # Input : X Featrue\n",
    "train_y = train_df.filter(regex='Y') # Output : Y Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QY7DfrtCV3vj",
    "outputId": "ce28163b-30e9-44d0-b89a-861b7a831f26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_04</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>X_10</th>\n",
       "      <th>...</th>\n",
       "      <th>X_47</th>\n",
       "      <th>X_48</th>\n",
       "      <th>X_49</th>\n",
       "      <th>X_50</th>\n",
       "      <th>X_51</th>\n",
       "      <th>X_52</th>\n",
       "      <th>X_53</th>\n",
       "      <th>X_54</th>\n",
       "      <th>X_55</th>\n",
       "      <th>X_56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.544</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.47</td>\n",
       "      <td>1</td>\n",
       "      <td>101.892</td>\n",
       "      <td>74.983</td>\n",
       "      <td>29.45</td>\n",
       "      <td>62.38</td>\n",
       "      <td>245.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9706.03</td>\n",
       "      <td>137.043591</td>\n",
       "      <td>135.359219</td>\n",
       "      <td>147.837968</td>\n",
       "      <td>134.313475</td>\n",
       "      <td>125.605427</td>\n",
       "      <td>136.721425</td>\n",
       "      <td>125.028256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.321</td>\n",
       "      <td>65.17</td>\n",
       "      <td>1</td>\n",
       "      <td>101.944</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.73</td>\n",
       "      <td>61.23</td>\n",
       "      <td>233.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10423.43</td>\n",
       "      <td>133.736691</td>\n",
       "      <td>135.979817</td>\n",
       "      <td>149.924692</td>\n",
       "      <td>123.630583</td>\n",
       "      <td>127.893337</td>\n",
       "      <td>143.322659</td>\n",
       "      <td>124.877308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.583</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.07</td>\n",
       "      <td>1</td>\n",
       "      <td>103.153</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.81</td>\n",
       "      <td>105.77</td>\n",
       "      <td>272.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10948.53</td>\n",
       "      <td>132.805112</td>\n",
       "      <td>131.055355</td>\n",
       "      <td>146.814592</td>\n",
       "      <td>128.939070</td>\n",
       "      <td>127.012195</td>\n",
       "      <td>140.395688</td>\n",
       "      <td>122.238232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.563</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.57</td>\n",
       "      <td>1</td>\n",
       "      <td>101.971</td>\n",
       "      <td>77.022</td>\n",
       "      <td>28.92</td>\n",
       "      <td>115.21</td>\n",
       "      <td>255.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15007.03</td>\n",
       "      <td>134.138760</td>\n",
       "      <td>133.239422</td>\n",
       "      <td>139.720132</td>\n",
       "      <td>132.260824</td>\n",
       "      <td>130.723186</td>\n",
       "      <td>147.624829</td>\n",
       "      <td>134.875225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.320</td>\n",
       "      <td>63.57</td>\n",
       "      <td>1</td>\n",
       "      <td>101.981</td>\n",
       "      <td>70.904</td>\n",
       "      <td>29.68</td>\n",
       "      <td>103.38</td>\n",
       "      <td>241.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11051.03</td>\n",
       "      <td>142.728970</td>\n",
       "      <td>136.620022</td>\n",
       "      <td>134.853555</td>\n",
       "      <td>134.760252</td>\n",
       "      <td>125.647793</td>\n",
       "      <td>139.331105</td>\n",
       "      <td>123.272762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39602</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>62.27</td>\n",
       "      <td>1</td>\n",
       "      <td>103.150</td>\n",
       "      <td>66.825</td>\n",
       "      <td>30.20</td>\n",
       "      <td>77.83</td>\n",
       "      <td>298.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60630.73</td>\n",
       "      <td>129.965741</td>\n",
       "      <td>130.807148</td>\n",
       "      <td>133.481737</td>\n",
       "      <td>125.273130</td>\n",
       "      <td>121.780933</td>\n",
       "      <td>133.780110</td>\n",
       "      <td>129.029812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.321</td>\n",
       "      <td>62.77</td>\n",
       "      <td>1</td>\n",
       "      <td>102.021</td>\n",
       "      <td>66.825</td>\n",
       "      <td>29.21</td>\n",
       "      <td>102.25</td>\n",
       "      <td>270.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60763.43</td>\n",
       "      <td>127.633885</td>\n",
       "      <td>120.158764</td>\n",
       "      <td>142.667802</td>\n",
       "      <td>122.465490</td>\n",
       "      <td>122.987209</td>\n",
       "      <td>143.090741</td>\n",
       "      <td>122.811413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>68.504</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.67</td>\n",
       "      <td>1</td>\n",
       "      <td>103.144</td>\n",
       "      <td>68.864</td>\n",
       "      <td>29.96</td>\n",
       "      <td>102.61</td>\n",
       "      <td>198.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8813.33</td>\n",
       "      <td>132.501286</td>\n",
       "      <td>136.893025</td>\n",
       "      <td>134.419328</td>\n",
       "      <td>129.115431</td>\n",
       "      <td>130.920147</td>\n",
       "      <td>140.489232</td>\n",
       "      <td>119.166699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>63.67</td>\n",
       "      <td>1</td>\n",
       "      <td>102.025</td>\n",
       "      <td>67.845</td>\n",
       "      <td>30.30</td>\n",
       "      <td>112.60</td>\n",
       "      <td>275.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62222.33</td>\n",
       "      <td>128.189679</td>\n",
       "      <td>121.495930</td>\n",
       "      <td>141.288011</td>\n",
       "      <td>130.141676</td>\n",
       "      <td>125.518825</td>\n",
       "      <td>136.603634</td>\n",
       "      <td>124.525929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>65.67</td>\n",
       "      <td>1</td>\n",
       "      <td>102.004</td>\n",
       "      <td>69.884</td>\n",
       "      <td>30.16</td>\n",
       "      <td>112.90</td>\n",
       "      <td>276.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62172.23</td>\n",
       "      <td>135.096272</td>\n",
       "      <td>122.988476</td>\n",
       "      <td>142.019357</td>\n",
       "      <td>123.752157</td>\n",
       "      <td>130.648365</td>\n",
       "      <td>139.695370</td>\n",
       "      <td>136.714504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39607 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_01     X_02   X_03  X_04     X_05    X_06   X_07    X_08    X_09  \\\n",
       "0      70.544  103.320  67.47     1  101.892  74.983  29.45   62.38  245.71   \n",
       "1      69.524  103.321  65.17     1  101.944  72.943  28.73   61.23  233.61   \n",
       "2      72.583  103.320  64.07     1  103.153  72.943  28.81  105.77  272.20   \n",
       "3      71.563  103.320  67.57     1  101.971  77.022  28.92  115.21  255.36   \n",
       "4      69.524  103.320  63.57     1  101.981  70.904  29.68  103.38  241.46   \n",
       "...       ...      ...    ...   ...      ...     ...    ...     ...     ...   \n",
       "39602  66.465  103.320  62.27     1  103.150  66.825  30.20   77.83  298.05   \n",
       "39603  66.465  103.321  62.77     1  102.021  66.825  29.21  102.25  270.67   \n",
       "39604  68.504  103.320  64.67     1  103.144  68.864  29.96  102.61  198.07   \n",
       "39605  66.465  103.320  63.67     1  102.025  67.845  30.30  112.60  275.52   \n",
       "39606  66.465  103.320  65.67     1  102.004  69.884  30.16  112.90  276.06   \n",
       "\n",
       "       X_10  ...  X_47  X_48      X_49        X_50        X_51        X_52  \\\n",
       "0       0.0  ...     1     1   9706.03  137.043591  135.359219  147.837968   \n",
       "1       0.0  ...     1     1  10423.43  133.736691  135.979817  149.924692   \n",
       "2       0.0  ...     1     1  10948.53  132.805112  131.055355  146.814592   \n",
       "3       0.0  ...     1     1  15007.03  134.138760  133.239422  139.720132   \n",
       "4       0.0  ...     1     1  11051.03  142.728970  136.620022  134.853555   \n",
       "...     ...  ...   ...   ...       ...         ...         ...         ...   \n",
       "39602   0.0  ...     1     1  60630.73  129.965741  130.807148  133.481737   \n",
       "39603   0.0  ...     1     1  60763.43  127.633885  120.158764  142.667802   \n",
       "39604   0.0  ...     1     1   8813.33  132.501286  136.893025  134.419328   \n",
       "39605   0.0  ...     1     1  62222.33  128.189679  121.495930  141.288011   \n",
       "39606   0.0  ...     1     1  62172.23  135.096272  122.988476  142.019357   \n",
       "\n",
       "             X_53        X_54        X_55        X_56  \n",
       "0      134.313475  125.605427  136.721425  125.028256  \n",
       "1      123.630583  127.893337  143.322659  124.877308  \n",
       "2      128.939070  127.012195  140.395688  122.238232  \n",
       "3      132.260824  130.723186  147.624829  134.875225  \n",
       "4      134.760252  125.647793  139.331105  123.272762  \n",
       "...           ...         ...         ...         ...  \n",
       "39602  125.273130  121.780933  133.780110  129.029812  \n",
       "39603  122.465490  122.987209  143.090741  122.811413  \n",
       "39604  129.115431  130.920147  140.489232  119.166699  \n",
       "39605  130.141676  125.518825  136.603634  124.525929  \n",
       "39606  123.752157  130.648365  139.695370  136.714504  \n",
       "\n",
       "[39607 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTKl6ejLV3vu",
    "outputId": "fbaf9f55-f301-4b64-bf99-31f2e9f15fda"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "100%|██████████| 54/54 [00:41<00:00,  1.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_04</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>X_12</th>\n",
       "      <th>...</th>\n",
       "      <th>kf_X_44</th>\n",
       "      <th>kf_X_45</th>\n",
       "      <th>kf_X_46</th>\n",
       "      <th>kf_X_47</th>\n",
       "      <th>kf_X_48</th>\n",
       "      <th>kf_X_49</th>\n",
       "      <th>kf_X_50</th>\n",
       "      <th>kf_X_51</th>\n",
       "      <th>kf_X_52</th>\n",
       "      <th>kf_X_53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.544</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.47</td>\n",
       "      <td>1</td>\n",
       "      <td>101.892</td>\n",
       "      <td>74.983</td>\n",
       "      <td>29.45</td>\n",
       "      <td>62.38</td>\n",
       "      <td>245.71</td>\n",
       "      <td>4.34</td>\n",
       "      <td>...</td>\n",
       "      <td>1.002494</td>\n",
       "      <td>1.002494</td>\n",
       "      <td>9681.830424</td>\n",
       "      <td>136.706824</td>\n",
       "      <td>135.026652</td>\n",
       "      <td>147.474282</td>\n",
       "      <td>133.983516</td>\n",
       "      <td>125.297184</td>\n",
       "      <td>136.385461</td>\n",
       "      <td>124.721452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.321</td>\n",
       "      <td>65.17</td>\n",
       "      <td>1</td>\n",
       "      <td>101.944</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.73</td>\n",
       "      <td>61.23</td>\n",
       "      <td>233.61</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995193</td>\n",
       "      <td>0.995193</td>\n",
       "      <td>10463.125216</td>\n",
       "      <td>134.417826</td>\n",
       "      <td>136.614816</td>\n",
       "      <td>150.605471</td>\n",
       "      <td>124.370038</td>\n",
       "      <td>128.465304</td>\n",
       "      <td>143.906278</td>\n",
       "      <td>125.470122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.583</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.07</td>\n",
       "      <td>1</td>\n",
       "      <td>103.153</td>\n",
       "      <td>72.943</td>\n",
       "      <td>28.81</td>\n",
       "      <td>105.77</td>\n",
       "      <td>272.20</td>\n",
       "      <td>4.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>0.996717</td>\n",
       "      <td>11008.959730</td>\n",
       "      <td>132.863430</td>\n",
       "      <td>132.432688</td>\n",
       "      <td>148.165211</td>\n",
       "      <td>126.714975</td>\n",
       "      <td>127.943775</td>\n",
       "      <td>142.420532</td>\n",
       "      <td>123.066148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.563</td>\n",
       "      <td>103.320</td>\n",
       "      <td>67.57</td>\n",
       "      <td>1</td>\n",
       "      <td>101.971</td>\n",
       "      <td>77.022</td>\n",
       "      <td>28.92</td>\n",
       "      <td>115.21</td>\n",
       "      <td>255.36</td>\n",
       "      <td>4.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997523</td>\n",
       "      <td>0.997523</td>\n",
       "      <td>14001.464930</td>\n",
       "      <td>133.319729</td>\n",
       "      <td>132.551377</td>\n",
       "      <td>142.332612</td>\n",
       "      <td>129.976112</td>\n",
       "      <td>130.279945</td>\n",
       "      <td>146.807645</td>\n",
       "      <td>131.078737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.524</td>\n",
       "      <td>103.320</td>\n",
       "      <td>63.57</td>\n",
       "      <td>1</td>\n",
       "      <td>101.981</td>\n",
       "      <td>70.904</td>\n",
       "      <td>29.68</td>\n",
       "      <td>103.38</td>\n",
       "      <td>241.46</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998013</td>\n",
       "      <td>0.998013</td>\n",
       "      <td>12899.355654</td>\n",
       "      <td>138.704302</td>\n",
       "      <td>134.670775</td>\n",
       "      <td>136.901889</td>\n",
       "      <td>132.935585</td>\n",
       "      <td>128.206028</td>\n",
       "      <td>143.653289</td>\n",
       "      <td>127.597410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39602</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>62.27</td>\n",
       "      <td>1</td>\n",
       "      <td>103.150</td>\n",
       "      <td>66.825</td>\n",
       "      <td>30.20</td>\n",
       "      <td>77.83</td>\n",
       "      <td>298.05</td>\n",
       "      <td>4.36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56282.257274</td>\n",
       "      <td>129.604506</td>\n",
       "      <td>125.719101</td>\n",
       "      <td>137.318833</td>\n",
       "      <td>124.581296</td>\n",
       "      <td>125.063380</td>\n",
       "      <td>137.602621</td>\n",
       "      <td>128.758425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.321</td>\n",
       "      <td>62.77</td>\n",
       "      <td>1</td>\n",
       "      <td>102.021</td>\n",
       "      <td>66.825</td>\n",
       "      <td>29.21</td>\n",
       "      <td>102.25</td>\n",
       "      <td>270.67</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58640.674456</td>\n",
       "      <td>129.265428</td>\n",
       "      <td>124.683345</td>\n",
       "      <td>138.073689</td>\n",
       "      <td>124.194959</td>\n",
       "      <td>124.566170</td>\n",
       "      <td>138.298172</td>\n",
       "      <td>127.727242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>68.504</td>\n",
       "      <td>103.320</td>\n",
       "      <td>64.67</td>\n",
       "      <td>1</td>\n",
       "      <td>103.144</td>\n",
       "      <td>68.864</td>\n",
       "      <td>29.96</td>\n",
       "      <td>102.61</td>\n",
       "      <td>198.07</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>52616.608456</td>\n",
       "      <td>129.711401</td>\n",
       "      <td>126.340585</td>\n",
       "      <td>137.494041</td>\n",
       "      <td>124.875757</td>\n",
       "      <td>125.353904</td>\n",
       "      <td>138.545457</td>\n",
       "      <td>126.227423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>63.67</td>\n",
       "      <td>1</td>\n",
       "      <td>102.025</td>\n",
       "      <td>67.845</td>\n",
       "      <td>30.30</td>\n",
       "      <td>112.60</td>\n",
       "      <td>275.52</td>\n",
       "      <td>4.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>55226.707986</td>\n",
       "      <td>129.456898</td>\n",
       "      <td>125.494027</td>\n",
       "      <td>138.027848</td>\n",
       "      <td>125.665171</td>\n",
       "      <td>125.255583</td>\n",
       "      <td>138.178005</td>\n",
       "      <td>125.697982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>66.465</td>\n",
       "      <td>103.320</td>\n",
       "      <td>65.67</td>\n",
       "      <td>1</td>\n",
       "      <td>102.004</td>\n",
       "      <td>69.884</td>\n",
       "      <td>30.16</td>\n",
       "      <td>112.90</td>\n",
       "      <td>276.06</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57515.806414</td>\n",
       "      <td>130.294870</td>\n",
       "      <td>124.957717</td>\n",
       "      <td>138.634630</td>\n",
       "      <td>125.401073</td>\n",
       "      <td>125.970258</td>\n",
       "      <td>138.325766</td>\n",
       "      <td>127.122784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39607 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X_01     X_02   X_03  X_04     X_05    X_06   X_07    X_08    X_09  \\\n",
       "0      70.544  103.320  67.47     1  101.892  74.983  29.45   62.38  245.71   \n",
       "1      69.524  103.321  65.17     1  101.944  72.943  28.73   61.23  233.61   \n",
       "2      72.583  103.320  64.07     1  103.153  72.943  28.81  105.77  272.20   \n",
       "3      71.563  103.320  67.57     1  101.971  77.022  28.92  115.21  255.36   \n",
       "4      69.524  103.320  63.57     1  101.981  70.904  29.68  103.38  241.46   \n",
       "...       ...      ...    ...   ...      ...     ...    ...     ...     ...   \n",
       "39602  66.465  103.320  62.27     1  103.150  66.825  30.20   77.83  298.05   \n",
       "39603  66.465  103.321  62.77     1  102.021  66.825  29.21  102.25  270.67   \n",
       "39604  68.504  103.320  64.67     1  103.144  68.864  29.96  102.61  198.07   \n",
       "39605  66.465  103.320  63.67     1  102.025  67.845  30.30  112.60  275.52   \n",
       "39606  66.465  103.320  65.67     1  102.004  69.884  30.16  112.90  276.06   \n",
       "\n",
       "       X_12  ...   kf_X_44   kf_X_45       kf_X_46     kf_X_47     kf_X_48  \\\n",
       "0      4.34  ...  1.002494  1.002494   9681.830424  136.706824  135.026652   \n",
       "1      4.38  ...  0.995193  0.995193  10463.125216  134.417826  136.614816   \n",
       "2      4.36  ...  0.996717  0.996717  11008.959730  132.863430  132.432688   \n",
       "3      4.33  ...  0.997523  0.997523  14001.464930  133.319729  132.551377   \n",
       "4      4.35  ...  0.998013  0.998013  12899.355654  138.704302  134.670775   \n",
       "...     ...  ...       ...       ...           ...         ...         ...   \n",
       "39602  4.36  ...  1.000000  1.000000  56282.257274  129.604506  125.719101   \n",
       "39603  4.40  ...  1.000000  1.000000  58640.674456  129.265428  124.683345   \n",
       "39604  4.38  ...  1.000000  1.000000  52616.608456  129.711401  126.340585   \n",
       "39605  4.33  ...  1.000000  1.000000  55226.707986  129.456898  125.494027   \n",
       "39606  4.38  ...  1.000000  1.000000  57515.806414  130.294870  124.957717   \n",
       "\n",
       "          kf_X_49     kf_X_50     kf_X_51     kf_X_52     kf_X_53  \n",
       "0      147.474282  133.983516  125.297184  136.385461  124.721452  \n",
       "1      150.605471  124.370038  128.465304  143.906278  125.470122  \n",
       "2      148.165211  126.714975  127.943775  142.420532  123.066148  \n",
       "3      142.332612  129.976112  130.279945  146.807645  131.078737  \n",
       "4      136.901889  132.935585  128.206028  143.653289  127.597410  \n",
       "...           ...         ...         ...         ...         ...  \n",
       "39602  137.318833  124.581296  125.063380  137.602621  128.758425  \n",
       "39603  138.073689  124.194959  124.566170  138.298172  127.727242  \n",
       "39604  137.494041  124.875757  125.353904  138.545457  126.227423  \n",
       "39605  138.027848  125.665171  125.255583  138.178005  125.697982  \n",
       "39606  138.634630  125.401073  125.970258  138.325766  127.122784  \n",
       "\n",
       "[39607 rows x 108 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbHGOGq3uIRJ"
   },
   "source": [
    "# Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiqY4EkTvmfv"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(20, 5, figsize=(30, 20))\n",
    "for var, subplot in zip(train_x.columns, ax.flatten()):\n",
    "    sns.scatterplot(x=var, y=train_y['Y_02'], data=train_x, ax=subplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Z98nnTfvmiV"
   },
   "outputs": [],
   "source": [
    "train_df['is_error'] = \"N\"\n",
    "train_df['is_error'][[0, 6, 9, 12, 14, 23, 33, 35, 37, 60, 67, 70, 72, 74, 76, 86, 98, 105, 120, 135, 137, 138, 140, 142, 144, 145, 147, 169, 177, 183, 186, 187, 201, 205, 207, 219, 222, 223, 226, 239, 240, 246, 253, 264, 267, 270, 275, 292, 299, 305, 315, 322, 328, 332, 333, 344, 345, 347, 353, 354, 355, 356, 357, 361, 364, 365, 367, 386, 396, 403, 406, 408, 410, 414, 415, 418, 425, 435, 440, 448, 451, 453, 454, 455, 458, 465, 469, 476, 484, 485, 494, 505, 531, 536, 537, 543, 555, 557, 576, 580, 593, 597, 599, 603, 610, 628, 638, 655, 656, 671, 672, 682, 683, 688, 689, 699, 711, 722, 730, 736, 746, 764, 765, 773, 778, 783, 787, 788, 796, 827, 833, 834, 864, 876, 881, 882, 885, 892, 910, 915, 921, 924, 927, 929, 930, 931, 932, 933, 938, 942, 943, 949, 951, 956, 957, 958, 967, 969, 970, 971, 972, 973, 976, 977, 979, 995, 999, 1020, 1031, 1032, 1043, 1045, 1050, 1053, 1064, 1074, 1078, 1082, 1084, 1091, 1104, 1106, 1107, 1108, 1116, 1123, 1127, 1131, 1140, 1144, 1145, 1153, 1154, 1167, 1189, 1208, 1220, 1232, 1248, 1263, 1267, 1273, 1283, 1286, 1287, 1295, 1311, 1312, 1327, 1328, 1346, 1356, 1358, 1366, 1371, 1372, 1379, 1386, 1390, 1397, 1398, 1421, 1422, 1425, 1426, 1435, 1436, 1453, 1458, 1459, 1462, 1471, 1476, 1479, 1485, 1491, 1500, 1503, 1504, 1505, 1519, 1521, 1545, 1575, 1588, 1603, 1609, 1636, 1637, 1645, 1649, 1657, 1659, 1663, 1683, 1706, 1707, 1725, 1741, 1784, 1819, 1832, 1849, 1854, 1855, 1859, 1872, 1960, 1961, 1963, 1971, 1984, 2007, 2014, 2048, 2068, 2081, 2083, 2085, 2100, 2128, 2145, 2157, 2166, 2170, 2215, 2228, 2231, 2236, 2263, 2307, 2319, 2334, 2361, 2363, 2367, 2374, 2378, 2379, 2382, 2385, 2394, 2395, 2407, 2420, 2430, 2436, 2460, 2465, 2476, 2493, 2500, 2555, 2560, 2578, 2584, 2606, 2618, 2621, 2623, 2625, 2635, 2669, 2675, 2680, 2683, 2685, 2687, 2699, 2705, 2714, 2715, 2717, 2736, 2748, 2754, 2767, 2777, 2791, 2795, 2796, 2797, 2800, 2803, 2806, 2808, 2816, 2827, 2831, 2832, 2833, 2841, 2848, 2858, 2869, 2877, 2880, 2886, 2887, 2895, 2910, 2916, 2918, 2928, 2940, 2947, 2948, 2953, 2959, 2960, 2961, 2964, 2977, 2978, 2980, 2993, 3005, 3013, 3014, 3022, 3026, 3031, 3038, 3055, 3057, 3060, 3070, 3077, 3084, 3087, 3111, 3112, 3121, 3130, 3164, 3172, 3176, 3177, 3178, 3182, 3183, 3193, 3196, 3201, 3217, 3229, 3249, 3251, 3253, 3254, 3262, 3276, 3291, 3299, 3306, 3317, 3341, 3347, 3348, 3356, 3363, 3373, 3374, 3385, 3388, 3391, 3402, 3404, 3415, 3427, 3453, 3454, 3456, 3464, 3492, 3497, 3504, 3523, 3570, 3579, 3607, 3626, 3640, 3643, 3653, 3663, 3702, 3714, 3722, 3724, 3744, 3754, 3768, 3771, 3777, 3778, 3790, 3817, 3839, 3841, 3856, 3859, 3862, 3921, 3934, 3942, 3944, 3947, 3961, 3962, 3965, 3966, 3975, 3979, 3982, 3990, 3994, 4018, 4036, 4058, 4066, 4108, 4109, 4118, 4128, 4144, 4150, 4151, 4153, 4165, 4172, 4207, 4215, 4251, 4258, 4259, 4263, 4264, 4268, 4272, 4301, 4316, 4317, 4321, 4327, 4330, 4357, 4383, 4399, 4405, 4413, 4419, 4438, 4457, 4464, 4467, 4516, 4533, 4535, 4549, 4574, 4580, 4583, 4584, 4603, 4628, 4630, 4636, 4638, 4644, 4659, 4669, 4676, 4680, 4684, 4688, 4696, 4697, 4707, 4715, 4717, 4720, 4728, 4730, 4762, 4769, 4777, 4779, 4791, 4793, 4796, 4805, 4808, 4832, 4834, 4836, 4838, 4846, 4863, 4869, 4877, 4879, 4880, 4882, 4883, 4885, 4893, 4901, 4902, 4914, 4916, 4920, 4925, 4930, 4938, 4950, 4953, 4959, 4960, 4975, 4987, 4990, 4991, 5019, 5034, 5038, 5039, 5040, 5042, 5043, 5044, 5048, 5051, 5053, 5061, 5066, 5075, 5077, 5092, 5099, 5104, 5108, 5119, 5125, 5131, 5133, 5134, 5137, 5138, 5144, 5176, 5185, 5188, 5193, 5195, 5200, 5210, 5212, 5218, 5252, 5256, 5262, 5272, 5286, 5294, 5295, 5296, 5297, 5303, 5306, 5317, 5326, 5331, 5332, 5361, 5364, 5388, 5397, 5398, 5401, 5404, 5406, 5410, 5419, 5420, 5430, 5471, 5476, 5477, 5484, 5494, 5498, 5499, 5508, 5515, 5518, 5532, 5539, 5540, 5541, 5543, 5547, 5576, 5577, 5578, 5598, 5599, 5600, 5601, 5603, 5604, 5606, 5610, 5626, 5628, 5639, 5640, 5642, 5643, 5644, 5645, 5649, 5650, 5651, 5652, 5653, 5655, 5656, 5660, 5661, 5668, 5706, 5708, 5720, 5729, 5773, 5775, 5783, 5787, 5797, 5810, 5812, 5816, 5823, 5829, 5842, 5857, 5890, 5892, 5905, 5933, 5940, 5943, 5949, 5968, 5970, 5979, 6009, 6017, 6028, 6047, 6049, 6050, 6062, 6088, 6103, 6111, 6114, 6141, 6163, 6169, 6173, 6178, 6187, 6206, 6219, 6237, 6239, 6264, 6271, 6276, 6279, 6286, 6309, 6317, 6318, 6341, 6342, 6350, 6353, 6356, 6364, 6367, 6371, 6378, 6387, 6397, 6411, 6422, 6436, 6478, 6486, 6489, 6550, 6559, 6562, 6568, 6576, 6585, 6586, 6622, 6654, 6656, 6657, 6711, 6716, 6727, 6764, 6786, 6790, 6794, 6796, 6806, 6813, 6816, 6847, 6866, 6868, 6871, 6876, 6878, 6934, 6938, 6965, 6983, 6986, 6991, 6993, 6998, 6999, 7026, 7028, 7029, 7031, 7049, 7054, 7089, 7090, 7094, 7095, 7096, 7107, 7109, 7116, 7134, 7137, 7141, 7146, 7150, 7161, 7186, 7188, 7195, 7202, 7207, 7208, 7210, 7211, 7233, 7247, 7255, 7258, 7262, 7264, 7271, 7279, 7287, 7289, 7299, 7325, 7331, 7336, 7353, 7355, 7358, 7359, 7370, 7379, 7402, 7403, 7405, 7411, 7413, 7435, 7436, 7440, 7441, 7442, 7443, 7444, 7446, 7447, 7451, 7455, 7457, 7465, 7466, 7473, 7475, 7482, 7494, 7497, 7501, 7505, 7507, 7511, 7517, 7528, 7541, 7558, 7582, 7583, 7587, 7612, 7620, 7625, 7629, 7631, 7657, 7660, 7665, 7690, 7733, 7737, 7741, 7743, 7746, 7749, 7762, 7768, 7772, 7773, 7780, 7793, 7797, 7812, 7825, 7831, 7833, 7835, 7843, 7844, 7845, 7846, 7853, 7856, 7859, 7894, 7895, 7903, 7905, 7907, 7909, 7910, 7912, 7914, 7917, 7920, 7925, 7926, 7943, 7945, 7946, 7955, 7961, 7965, 7968, 7980, 7981, 7983, 7987, 7997, 7999, 8010, 8012, 8019, 8030, 8037, 8050, 8055, 8060, 8064, 8069, 8085, 8090, 8103, 8109, 8112, 8127, 8170, 8183, 8216, 8217, 8231, 8242, 8251, 8276, 8298, 8302, 8304, 8318, 8328, 8329, 8353, 8361, 8370, 8390, 8393, 8401, 8405, 8408, 8411, 8439, 8445, 8452, 8454, 8458, 8466, 8468, 8469, 8483, 8489, 8498, 8499, 8512, 8523, 8525, 8554, 8562, 8565, 8587, 8588, 8599, 8600, 8602, 8608, 8615, 8621, 8636, 8658, 8674, 8691, 8707, 8715, 8733, 8736, 8739, 8758, 8761, 8774, 8788, 8792, 8794, 8796, 8811, 8823, 8830, 8838, 8928, 8945, 8947, 8952, 8969, 8984, 9029, 9052, 9095, 9097, 9145, 9147, 9151, 9163, 9170, 9200, 9214, 9218, 9229, 9247, 9250, 9259, 9291, 9297, 9302, 9303, 9317, 9318, 9323, 9324, 9327, 9330, 9336, 9363, 9369, 9374, 9389, 9390, 9394, 9395, 9416, 9431, 9448, 9450, 9471, 9485, 9488, 9494, 9502, 9503, 9504, 9507, 9526, 9529, 9545, 9553, 9591, 9625, 9627, 9640, 9644, 9645, 9648, 9653, 9654, 9655, 9658, 9659, 9662, 9665, 9671, 9700, 9707, 9718, 9727, 9729, 9733, 9740, 9741, 9757, 9758, 9760, 9762, 9782, 9796, 9801, 9805, 9826, 9834, 9840, 9842, 9845, 9850, 9851, 9852, 9869, 9888, 9890, 9901, 9902, 9905, 9908, 9940, 9951, 9965, 9968, 9969, 9973, 9982, 9983, 9989, 9999, 10000, 10007, 10008, 10040, 10043, 10050, 10065, 10068, 10082, 10121, 10122, 10127, 10129, 10136, 10149, 10161, 10178, 10182, 10189, 10193, 10203, 10229, 10253, 10254, 10255, 10257, 10260, 10263, 10267, 10273, 10278, 10280, 10282, 10283, 10284, 10285, 10286, 10288, 10289, 10292, 10307, 10316, 10330, 10332, 10341, 10357, 10373, 10379, 10380, 10386, 10388, 10390, 10392, 10398, 10406, 10413, 10420, 10436, 10438, 10440, 10444, 10460, 10470, 10490, 10493, 10501, 10510, 10527, 10546, 10568, 10582, 10583, 10591, 10596, 10598, 10599, 10623, 10638, 10643, 10646, 10693, 10740, 10748, 10763, 10767, 10785, 10792, 10796, 10816, 10823, 10832, 10833, 10842, 10846, 10848, 10855, 10895, 10907, 10929, 10935, 10936, 10947, 10955, 10985, 10987, 10989, 11020, 11044, 11045, 11046, 11051, 11073, 11075, 11096, 11119, 11135, 11148, 11154, 11167, 11169, 11196, 11214, 11233, 11265, 11275, 11300, 11321, 11375, 11376, 11380, 11412, 11442, 11468, 11494, 11503, 11508, 11516, 11519, 11588, 11589, 11594, 11621, 11626, 11627, 11632, 11642, 11646, 11647, 11662, 11664, 11687, 11695, 11703, 11731, 11732, 11735, 11737, 11743, 11748, 11754, 11769, 11772, 11786, 11793, 11796, 11806, 11820, 11822, 11825, 11828, 11840, 11845, 11862, 11866, 11872, 11877, 11882, 11887, 11892, 11903, 11915, 11927, 11929, 11939, 11947, 11968, 11970, 11996, 11997, 12006, 12008, 12010, 12011, 12016, 12019, 12027, 12035, 12038, 12043, 12046, 12050, 12074, 12083, 12091, 12093, 12094, 12096, 12098, 12104, 12121, 12130, 12135, 12137, 12145, 12166, 12168, 12181, 12190, 12198, 12204, 12217, 12221, 12258, 12272, 12273, 12283, 12309, 12319, 12322, 12323, 12354, 12359, 12363, 12373, 12374, 12378, 12384, 12386, 12394, 12405, 12406, 12428, 12433, 12434, 12450, 12453, 12455, 12490, 12499, 12500, 12504, 12508, 12510, 12531, 12535, 12544, 12547, 12548, 12554, 12569, 12583, 12603, 12604, 12605, 12606, 12607, 12609, 12612, 12615, 12616, 12617, 12618, 12625, 12627, 12629, 12630, 12653, 12654, 12655, 12659, 12661, 12663, 12677, 12679, 12719, 12730, 12750, 12761, 12769, 12784, 12812, 12871, 12872, 12877, 12911, 12949, 12964, 12983, 13020, 13046, 13047, 13050, 13056, 13066, 13068, 13089, 13098, 13104, 13111, 13141, 13159, 13172, 13174, 13184, 13192, 13210, 13219, 13224, 13228, 13230, 13235, 13258, 13259, 13267, 13278, 13280, 13306, 13311, 13312, 13313, 13321, 13324, 13330, 13339, 13358, 13367, 13368, 13369, 13373, 13374, 13376, 13392, 13407, 13411, 13437, 13438, 13445, 13451, 13452, 13456, 13475, 13476, 13503, 13507, 13517, 13525, 13537, 13551, 13556, 13578, 13584, 13614, 13627, 13645, 13683, 13684, 13686, 13700, 13754, 13759, 13798, 13849, 13894, 13920, 13923, 13960, 13962, 13987, 14010, 14019, 14021, 14031, 14049, 14067, 14077, 14101, 14114, 14129, 14165, 14172, 14236, 14246, 14250, 14280, 14301, 14328, 14329, 14356, 14383, 14387, 14398, 14410, 14418, 14423, 14440, 14443, 14444, 14453, 14460, 14462, 14487, 14488, 14499, 14502, 14504, 14517, 14530, 14535, 14536, 14558, 14566, 14584, 14588, 14593, 14615, 14622, 14629, 14632, 14636, 14651, 14652, 14673, 14674, 14678, 14684, 14701, 14705, 14712, 14719, 14724, 14735, 14739, 14767, 14782, 14788, 14803, 14818, 14824, 14833, 14840, 14844, 14854, 14857, 14868, 14885, 14911, 14915, 14916, 14924, 14929, 14931, 14933, 14935, 14940, 14945, 14974, 14976, 14984, 14995, 15013, 15014, 15025, 15028, 15034, 15036, 15045, 15051, 15062, 15064, 15082, 15084, 15100, 15120, 15130, 15138, 15147, 15154, 15158, 15178, 15181, 15193, 15194, 15198, 15204, 15209, 15210, 15241, 15246, 15248, 15252, 15253, 15281, 15287, 15289, 15308, 15330, 15332, 15334, 15335, 15336, 15339, 15347, 15351, 15352, 15357, 15359, 15363, 15365, 15367, 15371, 15375, 15382, 15383, 15399, 15414, 15424, 15427, 15428, 15431, 15433, 15440, 15443, 15447, 15449, 15453, 15454, 15459, 15464, 15467, 15471, 15486, 15492, 15495, 15496, 15503, 15504, 15505, 15506, 15508, 15520, 15526, 15531, 15532, 15535, 15536, 15540, 15545, 15548, 15554, 15560, 15561, 15564, 15566, 15568, 15569, 15572, 15579, 15588, 15592, 15594, 15603, 15608, 15617, 15627, 15629, 15630, 15634, 15644, 15645, 15666, 15674, 15675, 15677, 15678, 15681, 15696, 15704, 15709, 15710, 15720, 15745, 15755, 15759, 15772, 15776, 15795, 15812, 15835, 15863, 15869, 15870, 15898, 15900, 15908, 15912, 15944, 15951, 15952, 15965, 15967, 15971, 15977, 15978, 15985, 15986, 16004, 16009, 16013, 16068, 16078, 16094, 16098, 16108, 16110, 16114, 16116, 16124, 16126, 16129, 16132, 16143, 16146, 16160, 16163, 16186, 16217, 16219, 16220, 16228, 16231, 16234, 16239, 16247, 16254, 16255, 16263, 16270, 16273, 16274, 16276, 16278, 16284, 16289, 16292, 16294, 16304, 16329, 16336, 16351, 16356, 16357, 16359, 16369, 16371, 16393, 16398, 16401, 16405, 16406, 16418, 16425, 16452, 16458, 16499, 16502, 16515, 16517, 16522, 16528, 16565, 16575, 16595, 16600, 16646, 16650, 16659, 16665, 16680, 16683, 16689, 16690, 16701, 16705, 16710, 16711, 16714, 16717, 16729, 16734, 16735, 16740, 16749, 16760, 16770, 16788, 16810, 16812, 16815, 16828, 16833, 16837, 16839, 16843, 16855, 16871, 16881, 16905, 16906, 16913, 16931, 16944, 16954, 16957, 16962, 16965, 16970, 16977, 16995, 17004, 17012, 17019, 17022, 17030, 17034, 17040, 17052, 17061, 17066, 17067, 17100, 17113, 17145, 17148, 17159, 17182, 17208, 17218, 17223, 17229, 17233, 17240, 17243, 17252, 17258, 17259, 17279, 17282, 17285, 17289, 17292, 17293, 17311, 17314, 17336, 17341, 17354, 17357, 17366, 17374, 17376, 17382, 17395, 17415, 17419, 17429, 17430, 17438, 17448, 17454, 17458, 17459, 17467, 17469, 17474, 17482, 17485, 17489, 17493, 17508, 17519, 17523, 17527, 17540, 17544, 17549, 17568, 17569, 17590, 17595, 17597, 17612, 17624, 17626, 17628, 17636, 17640, 17647, 17655, 17678, 17684, 17687, 17696, 17705, 17707, 17719, 17722, 17725, 17728, 17749, 17750, 17764, 17776, 17784, 17792, 17797, 17810, 17817, 17850, 17861, 17890, 17897, 17932, 17937, 17992, 17994, 17995, 18004, 18018, 18028, 18035, 18041, 18049, 18054, 18103, 18105, 18108, 18118, 18124, 18134, 18174, 18182, 18191, 18220, 18270, 18291, 18336, 18353, 18368, 18397, 18433, 18453, 18456, 18505, 18507, 18511, 18521, 18536, 18540, 18565, 18593, 18601, 18612, 18631, 18633, 18639, 18683, 18684, 18686, 18703, 18704, 18724, 18748, 18781, 18800, 18805, 18812, 18823, 18826, 18834, 18841, 18847, 18848, 18850, 18855, 18864, 18902, 18912, 18926, 18942, 18958, 18980, 18994, 19007, 19012, 19026, 19031, 19037, 19038, 19040, 19042, 19067, 19071, 19102, 19129, 19141, 19142, 19167, 19170, 19186, 19191, 19206, 19216, 19218, 19231, 19244, 19249, 19264, 19305, 19317, 19321, 19328, 19339, 19359, 19367, 19395, 19414, 19430, 19433, 19469, 19482, 19485, 19492, 19495, 19517, 19521, 19524, 19526, 19529, 19534, 19540, 19546, 19552, 19560, 19561, 19567, 19572, 19576, 19614, 19615, 19626, 19629, 19634, 19662, 19665, 19673, 19685, 19686, 19688, 19690, 19696, 19710, 19711, 19714, 19727, 19729, 19730, 19734, 19737, 19738, 19741, 19755, 19764, 19768, 19769, 19792, 19794, 19797, 19806, 19812, 19813, 19819, 19823, 19826, 19833, 19835, 19836, 19871, 19880, 19925, 19940, 19949, 19961, 19968, 19989, 20000, 20016, 20033, 20040, 20041, 20044, 20099, 20101, 20125, 20134, 20135, 20144, 20145, 20163, 20168, 20184, 20192, 20217, 20218, 20219, 20252, 20260, 20299, 20307, 20308, 20318, 20319, 20332, 20333, 20347, 20348, 20349, 20351, 20362, 20369, 20386, 20387, 20391, 20396, 20406, 20413, 20415, 20437, 20463, 20492, 20495, 20503, 20510, 20517, 20524, 20543, 20551, 20566, 20568, 20569, 20580, 20584, 20585, 20607, 20608, 20619, 20624, 20638, 20649, 20656, 20673, 20674, 20710, 20712, 20728, 20735, 20785, 20797, 20829, 20835, 20857, 20859, 20863, 20881, 20901, 20910, 20911, 20948, 20954, 20974, 20986, 20998, 21007, 21047, 21089, 21095, 21110, 21115, 21139, 21148, 21156, 21172, 21177, 21190, 21219, 21235, 21237, 21249, 21254, 21256, 21268, 21282, 21306, 21328, 21364, 21365, 21370, 21387, 21391, 21395, 21409, 21416, 21453, 21457, 21472, 21477, 21515, 21516, 21520, 21522, 21525, 21528, 21536, 21545, 21548, 21552, 21565, 21572, 21586, 21587, 21606, 21615, 21645, 21654, 21665, 21672, 21681, 21685, 21686, 21690, 21691, 21734, 21767, 21771, 21776, 21777, 21801, 21813, 21820, 21837, 21838, 21844, 21850, 21860, 21861, 21863, 21864, 21869, 21870, 21872, 21882, 21890, 21901, 21914, 21947, 21953, 21960, 21968, 21971, 21977, 21988, 22002, 22015, 22020, 22022, 22031, 22033, 22034, 22036, 22038, 22047, 22075, 22091, 22097, 22106, 22112, 22116, 22117, 22121, 22125, 22165, 22168, 22172, 22185, 22193, 22199, 22212, 22233, 22234, 22246, 22247, 22292, 22317, 22340, 22342, 22353, 22355, 22381, 22410, 22416, 22420, 22421, 22423, 22430, 22441, 22468, 22474, 22478, 22505, 22513, 22516, 22526, 22532, 22536, 22565, 22588, 22595, 22609, 22616, 22625, 22626, 22649, 22664, 22700, 22714, 22733, 22763, 22775, 22786, 22795, 22801, 22802, 22807, 22816, 22835, 22866, 22894, 22898, 22901, 22902, 22912, 22919, 22946, 22950, 22953, 23000, 23027, 23029, 23039, 23047, 23049, 23071, 23120, 23173, 23175, 23188, 23190, 23213, 23215, 23217, 23232, 23238, 23265, 23280, 23304, 23306, 23313, 23321, 23340, 23352, 23357, 23374, 23395, 23412, 23419, 23427, 23464, 23477, 23505, 23527, 23540, 23586, 23589, 23622, 23654, 23659, 23693, 23695, 23716, 23726, 23771, 23778, 23785, 23787, 23795, 23799, 23811, 23815, 23818, 23828, 23829, 23832, 23834, 23836, 23846, 23849, 23850, 23855, 23856, 23866, 23868, 23879, 23881, 23889, 23910, 23917, 23919, 23922, 23962, 23980, 23995, 24027, 24028, 24031, 24042, 24043, 24045, 24056, 24067, 24072, 24080, 24085, 24094, 24097, 24098, 24101, 24107, 24119, 24128, 24145, 24148, 24182, 24200, 24213, 24223, 24230, 24238, 24247, 24248, 24254, 24274, 24311, 24314, 24322, 24335, 24339, 24340, 24342, 24348, 24355, 24376, 24396, 24398, 24403, 24408, 24409, 24420, 24428, 24445, 24466, 24479, 24480, 24496, 24497, 24503, 24507, 24529, 24540, 24552, 24557, 24564, 24567, 24578, 24593, 24607, 24651, 24655, 24660, 24661, 24693, 24700, 24722, 24725, 24732, 24734, 24737, 24739, 24743, 24744, 24747, 24750, 24764, 24799, 24801, 24805, 24810, 24820, 24827, 24848, 24858, 24860, 24861, 24862, 24899, 24919, 24923, 24939, 24954, 24959, 24961, 24963, 24987, 24991, 24992, 25000, 25011, 25042, 25047, 25048, 25059, 25061, 25067, 25104, 25107, 25110, 25111, 25116, 25122, 25131, 25139, 25140, 25141, 25142, 25145, 25148, 25149, 25151, 25155, 25172, 25179, 25180, 25185, 25186, 25195, 25197, 25229, 25237, 25247, 25258, 25270, 25325, 25347, 25371, 25388, 25393, 25425, 25450, 25456, 25461, 25481, 25521, 25526, 25535, 25547, 25561, 25601, 25618, 25644, 25657, 25660, 25671, 25673, 25681, 25684, 25730, 25733, 25736, 25763, 25768, 25781, 25783, 25795, 25798, 25807, 25824, 25852, 25853, 25873, 25874, 25876, 25877, 25879, 25881, 25909, 25930, 25936, 25947, 25964, 25986, 25988, 25997, 26000, 26007, 26011, 26020, 26027, 26034, 26043, 26048, 26050, 26058, 26059, 26064, 26068, 26107, 26110, 26114, 26121, 26135, 26140, 26164, 26166, 26209, 26213, 26216, 26219, 26234, 26241, 26249, 26267, 26288, 26289, 26291, 26303, 26306, 26312, 26338, 26347, 26349, 26354, 26355, 26378, 26379, 26383, 26388, 26390, 26395, 26415, 26417, 26418, 26429, 26440, 26458, 26468, 26478, 26497, 26506, 26511, 26522, 26526, 26538, 26551, 26554, 26557, 26559, 26585, 26590, 26596, 26606, 26619, 26653, 26667, 26676, 26756, 26809, 26843, 26844, 26853, 26860, 26869, 26885, 26886, 26924, 26934, 26936, 26942, 26980, 27003, 27008, 27010, 27044, 27046, 27054, 27062, 27071, 27076, 27114, 27125, 27126, 27130, 27143, 27148, 27155, 27156, 27162, 27163, 27166, 27184, 27189, 27196, 27201, 27205, 27247, 27253, 27257, 27258, 27268, 27270, 27271, 27275, 27276, 27279, 27283, 27292, 27344, 27345, 27346, 27361, 27379, 27386, 27401, 27411, 27416, 27420, 27446, 27448, 27451, 27452, 27473, 27476, 27480, 27484, 27493, 27503, 27506, 27509, 27530, 27544, 27547, 27551, 27552, 27560, 27564, 27572, 27592, 27600, 27609, 27624, 27654, 27663, 27666, 27674, 27679, 27685, 27690, 27701, 27702, 27709, 27715, 27716, 27730, 27731, 27735, 27738, 27740, 27755, 27762, 27766, 27769, 27779, 27784, 27799, 27817, 27825, 27828, 27834, 27845, 27849, 27885, 27886, 27890, 27913, 27914, 27925, 27930, 27939, 27950, 27951, 27956, 27958, 27971, 27977, 27997, 28002, 28008, 28012, 28023, 28024, 28038, 28039, 28040, 28041, 28042, 28057, 28060, 28067, 28071, 28086, 28092, 28093, 28101, 28107, 28116, 28118, 28122, 28138, 28142, 28147, 28163, 28164, 28166, 28167, 28171, 28187, 28189, 28191, 28206, 28212, 28214, 28215, 28218, 28223, 28231, 28235, 28236, 28247, 28254, 28263, 28264, 28271, 28273, 28274, 28277, 28279, 28283, 28289, 28290, 28297, 28302, 28305, 28318, 28334, 28335, 28337, 28341, 28353, 28358, 28359, 28360, 28367, 28368, 28380, 28381, 28399, 28411, 28416, 28420, 28422, 28426, 28433, 28439, 28461, 28462, 28470, 28482, 28494, 28497, 28498, 28499, 28506, 28509, 28526, 28543, 28544, 28546, 28554, 28580, 28660, 28667, 28668, 28714, 28717, 28728, 28734, 28738, 28740, 28762, 28773, 28820, 28826, 28831, 28847, 28852, 28862, 28864, 28869, 28874, 28876, 28880, 28898, 28935, 28963, 28965, 28969, 28975, 28991, 28998, 29002, 29010, 29012, 29020, 29025, 29026, 29027, 29040, 29052, 29053, 29057, 29058, 29066, 29072, 29077, 29078, 29080, 29103, 29108, 29110, 29115, 29118, 29125, 29136, 29137, 29141, 29143, 29144, 29150, 29154, 29193, 29194, 29231, 29262, 29275, 29288, 29296, 29297, 29300, 29328, 29332, 29346, 29347, 29349, 29351, 29381, 29382, 29394, 29418, 29425, 29432, 29447, 29458, 29467, 29471, 29481, 29484, 29485, 29492, 29497, 29499, 29501, 29514, 29525, 29533, 29539, 29566, 29568, 29569, 29583, 29630, 29638, 29639, 29651, 29675, 29680, 29687, 29713, 29725, 29726, 29734, 29743, 29758, 29790, 29798, 29806, 29808, 29836, 29844, 29854, 29867, 29868, 29869, 29870, 29921, 29925, 29942, 29958, 29960, 29969, 29973, 29979, 29994, 30072, 30082, 30093, 30106, 30107, 30132, 30145, 30147, 30188, 30191, 30193, 30202, 30219, 30230, 30231, 30235, 30250, 30251, 30260, 30261, 30272, 30277, 30284, 30286, 30295, 30296, 30306, 30310, 30317, 30323, 30341, 30345, 30352, 30360, 30366, 30367, 30373, 30374, 30378, 30383, 30390, 30396, 30401, 30404, 30413, 30417, 30428, 30430, 30456, 30460, 30464, 30475, 30494, 30496, 30497, 30502, 30535, 30538, 30546, 30564, 30565, 30569, 30574, 30588, 30590, 30626, 30630, 30637, 30655, 30656, 30678, 30686, 30693, 30709, 30716, 30722, 30751, 30754, 30756, 30757, 30764, 30778, 30779, 30785, 30791, 30816, 30827, 30839, 30848, 30868, 30880, 30896, 30897, 30907, 30910, 30921, 30928, 30933, 30942, 30944, 30948, 30972, 30983, 30993, 31007, 31013, 31025, 31036, 31045, 31060, 31063, 31087, 31091, 31154, 31155, 31157, 31165, 31172, 31182, 31188, 31207, 31225, 31236, 31237, 31247, 31248, 31269, 31290, 31313, 31341, 31348, 31359, 31391, 31419, 31420, 31422, 31432, 31447, 31453, 31454, 31458, 31464, 31478, 31488, 31506, 31507, 31514, 31515, 31516, 31517, 31533, 31560, 31568, 31582, 31586, 31610, 31619, 31634, 31641, 31642, 31643, 31644, 31645, 31648, 31658, 31665, 31667, 31709, 31716, 31720, 31725, 31726, 31748, 31768, 31777, 31797, 31801, 31838, 31843, 31846, 31884, 31887, 31893, 31895, 31900, 31904, 31905, 31932, 31974, 31976, 31989, 32009, 32011, 32013, 32028, 32052, 32065, 32075, 32088, 32124, 32125, 32128, 32133, 32140, 32161, 32164, 32179, 32190, 32193, 32194, 32196, 32218, 32234, 32252, 32271, 32278, 32287, 32289, 32290, 32319, 32324, 32325, 32339, 32340, 32346, 32347, 32348, 32360, 32367, 32375, 32386, 32395, 32409, 32416, 32417, 32418, 32421, 32426, 32448, 32463, 32473, 32487, 32506, 32507, 32509, 32518, 32522, 32528, 32540, 32542, 32548, 32549, 32555, 32560, 32561, 32565, 32578, 32584, 32592, 32598, 32599, 32603, 32605, 32606, 32607, 32608, 32622, 32624, 32626, 32631, 32636, 32642, 32651, 32661, 32697, 32711, 32715, 32724, 32754, 32787, 32812, 32848, 32849, 32892, 32906, 32907, 32910, 32913, 32928, 32950, 32978, 32985, 32989, 32991, 32992, 32998, 33028, 33039, 33043, 33063, 33094, 33109, 33120, 33124, 33141, 33147, 33155, 33159, 33175, 33185, 33192, 33204, 33205, 33210, 33212, 33213, 33221, 33228, 33233, 33240, 33243, 33247, 33263, 33266, 33269, 33272, 33273, 33278, 33290, 33291, 33296, 33312, 33323, 33354, 33358, 33382, 33386, 33414, 33422, 33424, 33438, 33440, 33442, 33458, 33459, 33461, 33518, 33540, 33549, 33553, 33583, 33584, 33586, 33597, 33637, 33648, 33656, 33658, 33692, 33723, 33738, 33746, 33758, 33760, 33784, 33791, 33803, 33827, 33828, 33830, 33848, 33901, 33930, 33988, 34020, 34046, 34049, 34060, 34082, 34088, 34103, 34142, 34157, 34162, 34164, 34165, 34178, 34187, 34191, 34203, 34224, 34264, 34272, 34285, 34287, 34299, 34316, 34356, 34361, 34376, 34380, 34431, 34457, 34482, 34502, 34505, 34507, 34508, 34517, 34538, 34542, 34557, 34564, 34574, 34576, 34578, 34600, 34635, 34654, 34655, 34659, 34661, 34679, 34684, 34686, 34715, 34727, 34730, 34733, 34740, 34746, 34748, 34771, 34775, 34788, 34790, 34793, 34800, 34821, 34857, 34861, 34867, 34874, 34901, 34910, 34917, 34920, 34930, 34944, 34946, 34966, 34973, 34985, 34999, 35030, 35043, 35051, 35052, 35056, 35069, 35072, 35074, 35081, 35097, 35121, 35124, 35126, 35134, 35144, 35153, 35225, 35239, 35241, 35258, 35261, 35293, 35296, 35312, 35338, 35341, 35343, 35351, 35363, 35370, 35381, 35411, 35456, 35529, 35536, 35544, 35556, 35629, 35647, 35670, 35674, 35684, 35697, 35714, 35726, 35769, 35819, 35833, 35835, 35836, 35857, 35862, 35868, 35892, 35913, 35947, 35984, 35986, 36020, 36059, 36084, 36116, 36118, 36139, 36142, 36147, 36161, 36166, 36176, 36200, 36206, 36214, 36219, 36231, 36274, 36283, 36284, 36294, 36321, 36335, 36341, 36350, 36355, 36362, 36364, 36365, 36384, 36398, 36416, 36420, 36438, 36442, 36494, 36515, 36532, 36541, 36543, 36594, 36615, 36619, 36627, 36664, 36672, 36673, 36674, 36678, 36684, 36699, 36707, 36722, 36724, 36732, 36743, 36754, 36757, 36763, 36765, 36767, 36769, 36807, 36819, 36826, 36834, 36844, 36849, 36851, 36863, 36879, 36888, 36901, 36915, 36930, 36942, 36949, 36955, 36965, 36978, 36980, 36986, 36992, 36997, 37003, 37006, 37015, 37020, 37021, 37029, 37030, 37032, 37036, 37054, 37060, 37065, 37095, 37097, 37099, 37100, 37118, 37138, 37144, 37146, 37155, 37174, 37205, 37216, 37232, 37236, 37245, 37263, 37312, 37313, 37347, 37358, 37377, 37402, 37413, 37422, 37425, 37431, 37443, 37461, 37462, 37466, 37473, 37475, 37495, 37497, 37511, 37524, 37525, 37531, 37537, 37539, 37548, 37552, 37554, 37559, 37563, 37567, 37573, 37582, 37590, 37591, 37593, 37605, 37606, 37610, 37619, 37625, 37638, 37644, 37651, 37659, 37670, 37672, 37677, 37711, 37721, 37731, 37750, 37755, 37765, 37771, 37779, 37782, 37829, 37844, 37863, 37916, 37922, 37951, 37958, 37970, 37993, 37995, 38001, 38019, 38027, 38031, 38036, 38058, 38060, 38061, 38101, 38102, 38113, 38134, 38157, 38171, 38173, 38180, 38192, 38206, 38213, 38231, 38277, 38297, 38362, 38368, 38386, 38413, 38430, 38457, 38473, 38478, 38488, 38518, 38521, 38528, 38535, 38580, 38592, 38605, 38667, 38672, 38678, 38699, 38702, 38717, 38725, 38746, 38752, 38755, 38759, 38764, 38765, 38774, 38779, 38780, 38784, 38785, 38786, 38787, 38793, 38814, 38816, 38818, 38831, 38835, 38854, 38876, 38893, 38899, 38913, 38927, 38930, 38947, 38978, 38979, 39017, 39056, 39062, 39082, 39091, 39109, 39126, 39127, 39132, 39133, 39137, 39139, 39157, 39170, 39232, 39239, 39245, 39246, 39257, 39258, 39267, 39283, 39309, 39319, 39324, 39330, 39332, 39352, 39363, 39378, 39381, 39383, 39393, 39401, 39411, 39415, 39418, 39427, 39434, 39465, 39470, 39478, 39486, 39488, 39503, 39504, 39510, 39527, 39533, 39559, 39569, 39571, 39583, 39596, 39605]] =\"Y\"\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3AoplyTcyz5"
   },
   "outputs": [],
   "source": [
    "specific = pd.read_csv(\"/content/gdrive/MyDrive/antena/data/meta/y_feature_spec_info.csv\")\n",
    "specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rk85om32Hkq9"
   },
   "outputs": [],
   "source": [
    "for i in range(len(specific)):\n",
    "    min = specific.iloc[i]['최소']\n",
    "    max = specific.iloc[i]['최대']\n",
    "    feature = specific.iloc[i]['Feature']\n",
    "\n",
    "    train_df[feature+\"_Error\"] = [\"Y\" if ((x<min) or (x>max)) else \"N\" for x in train_df[feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QiwQ0hUHkz4"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(base+\"df_new.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46P1Wc_St9ZW"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBi9xPP7t9cN"
   },
   "outputs": [],
   "source": [
    "train_x['Y_test'] = train_y['Y_01']\n",
    "train_x.corr().loc[\"Y_test\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYFTwVZcueUz"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BcbRG6vogzwZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "  0%|          | 0/54 [00:00<?, ?it/s]C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "100%|██████████| 54/54 [01:38<00:00,  1.83s/it]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x[col+\"_log\"] = train_x[col].apply(lambda x : math.log(x))\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['dif_'+a+b] = train_x[a]-train_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['dif_'+b+a] = train_x[b]-train_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['dif_'+a+b] = train_x[a]-train_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['dif_'+b+a] = train_x[b]-train_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['dif_'+a+b] = train_x[a]-train_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['dif_'+b+a] = train_x[b]-train_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['PCB_sum_1'] = train_x['X_01']+train_x['X_02']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['PCB_sum_2'] = train_x['X_05']+train_x['X_06']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['radi_sum_area'] = train_x['X_07'] + train_x['X_08']+ train_x['X_09']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['radi_1_weight_per_area'] = train_x['X_03'] / train_x['X_07']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['abs_X_41X_14'] =abs(train_x[\"X_41\"]- train_x['X_14'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['abs_X_42X_15'] =abs(train_x[\"X_42\"]- train_x['X_15'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['abs_X_43X_16'] =abs(train_x[\"X_43\"]- train_x['X_16'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3347139054.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_x['abs_X_44X_17'] =abs(train_x[\"X_44\"]- train_x['X_17'])\n"
     ]
    }
   ],
   "source": [
    "train_x.drop([\"X_10\",\"X_11\"], axis =1 ,inplace =True)\n",
    "\n",
    "basic_col = train_x.columns\n",
    "\n",
    "#for i in range(2,44):\n",
    "for i in tqdm(range(len(train_x.columns))):\n",
    "    current=0\n",
    "    sum_c=[]\n",
    "    z = train_x.loc[:, train_x.columns[i]]\n",
    "    a = []           #필터링 된 피쳐(after)\n",
    "    b = []           #필터링 전 피쳐(before)\n",
    "    my_filter = KalmanFilter(dim_x=2,dim_z=1) #create kalman filter\n",
    "    my_filter.x = np.array([[2.],[0.]])       # initial state (location and velocity)\n",
    "    my_filter.F = np.array([[1.,1.], [0.,1.]])    # state transition matrix\n",
    "    my_filter.H = np.array([[1.,0.]])    # Measurement function\n",
    "    my_filter.P *= 1000.                 # covariance matrix\n",
    "    my_filter.R = 5                      # state uncertainty\n",
    "    my_filter.Q = Q_discrete_white_noise(dim = 2,dt=.1,var=.1) # process uncertainty   \n",
    "    for k in z.values:\n",
    "        my_filter.predict()\n",
    "        my_filter.update(k)\n",
    "        # do something with the output\n",
    "        x = my_filter.x\n",
    "        a.extend(x[0])\n",
    "        b.append(k)\n",
    "    #dataset.loc[dataset.num == num, dataset.columns[i]] = a\n",
    "    sum_c=sum_c+a\n",
    "    train_x.loc[:,'kf_X_'+str(i)]=sum_c\n",
    "train_x\n",
    "\n",
    "\n",
    "temp_1 = ['X_14','X_15','X_16','X_17','X_18']\n",
    "temp_2 = ['X_19','X_20','X_21','X_22']\n",
    "temp_3 = ['X_30','X_31','X_32','X_33']\n",
    "\n",
    "for col in basic_col:\n",
    "    if col in ['X_38','X_39',\"X_40\",\"X_45\"]:\n",
    "        continue\n",
    "    train_x[col+\"_log\"] = train_x[col].apply(lambda x : math.log(x))\n",
    "\n",
    "\n",
    "\n",
    "# train_x[['X_04','X_23','X_47','X_48']] = train_x[['X_04','X_23','X_47','X_48']].astype(\"category\")\n",
    "for a,b in itertools.combinations(temp_1,2):\n",
    "    train_x['dif_'+a+b] = train_x[a]-train_x[b]\n",
    "    train_x['dif_'+b+a] = train_x[b]-train_x[a]\n",
    "    \n",
    "\n",
    "for a,b in itertools.combinations(temp_2,2):\n",
    "    train_x['dif_'+a+b] = train_x[a]-train_x[b]\n",
    "    train_x['dif_'+b+a] = train_x[b]-train_x[a]\n",
    " \n",
    "for a,b in itertools.combinations(temp_3,2):\n",
    "    train_x['dif_'+a+b] = train_x[a]-train_x[b]\n",
    "    train_x['dif_'+b+a] = train_x[b]-train_x[a]\n",
    "    \n",
    "\n",
    "train_x['PCB_sum_1'] = train_x['X_01']+train_x['X_02']\n",
    "train_x['PCB_sum_2'] = train_x['X_05']+train_x['X_06']\n",
    "train_x['radi_sum_area'] = train_x['X_07'] + train_x['X_08']+ train_x['X_09']\n",
    "train_x['radi_1_weight_per_area'] = train_x['X_03'] / train_x['X_07']\n",
    "# train_x['radi_2_weight_per_area'] = train_x['X_10'] / train_x['X_08']\n",
    "# train_x['radi_3_weight_per_area'] = train_x['X_11'] / train_x['X_09']\n",
    "train_x['abs_X_41X_14'] =abs(train_x[\"X_41\"]- train_x['X_14'])\n",
    "train_x['abs_X_42X_15'] =abs(train_x[\"X_42\"]- train_x['X_15'])\n",
    "train_x['abs_X_43X_16'] =abs(train_x[\"X_43\"]- train_x['X_16'])\n",
    "train_x['abs_X_44X_17'] =abs(train_x[\"X_44\"]- train_x['X_17'])\n",
    "\n",
    "\n",
    "train_x['var_antena_loc'] = train_x[['X_14','X_15','X_16','X_17',\"X_18\"]].apply(lambda x : np.var(x), axis =1)\n",
    "train_x['std_nthscrew_depth'] = train_x[['X_19','X_20','X_21','X_22']].apply(lambda x : np.std(x), axis =1)\n",
    "train_x['std_connector'] = train_x[['X_24','X_25','X_26','X_27',\"X_28\",\"X_29\"]].apply(lambda x : np.std(x), axis =1)\n",
    "train_x['std_screw_depth'] = train_x[['X_30','X_31','X_32','X_33']].apply(lambda x : np.std(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKEHZmCaXsVE"
   },
   "outputs": [],
   "source": [
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = mean_squared_error(gt.iloc[:,idx], preds.iloc[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt.iloc[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "        print(\"{}th's nrmse = {}\".format(idx, nrmse))\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU0B3Lk2Yg2b"
   },
   "source": [
    "# Modeling & Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsYnM7ziW1IB"
   },
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrYN9x5cuRPd"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SL7_ZOr7THro"
   },
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tkM1DehuxqB"
   },
   "source": [
    "##### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99MEgpodXsX5"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "def make_model_cat(k,train_x, train_y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_'+str(k)], test_size = 0.2, random_state=1422)\n",
    "    globals()['model_%s' %k] = catboost.CatBoostRegressor(loss_function='RMSE', use_best_model = True)\n",
    "    globals()['model_%s' %k].fit(X_train,y_train,\n",
    "           cat_features=['X_04','X_23','X_47','X_48'],\n",
    "          eval_set=(X_test, y_test),\n",
    "            use_best_model=True,\n",
    "            verbose = False,\n",
    "          plot=True)\n",
    "    y_pred = globals()['model_%s' %k].predict(X_test)\n",
    "    res.append(y_pred)\n",
    "    rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    print(\"rmse_{}= {}\".format(k, rmse))\n",
    "    return globals()['model_%s' %k]\n",
    "\n",
    "def make_model_cat_val(k,train_x, train_y, val_x, val_y):\n",
    "\n",
    "    globals()['model_%s' %k] = catboost.CatBoostRegressor(loss_function='RMSE', use_best_model = True)\n",
    "    globals()['model_%s' %k].fit(train_x,train_y['Y_'+str(k)],\n",
    "           cat_features=['X_04','X_23','X_47','X_48'],\n",
    "          eval_set=(val_x, val_y['Y_'+str(k)]),\n",
    "            use_best_model=True,\n",
    "            verbose = False,\n",
    "          plot=True)\n",
    "    y_pred = globals()['model_%s' %k].predict(val_x)\n",
    "    res.append(y_pred)\n",
    "    rmse = (np.sqrt(mean_squared_error(val_y['Y_'+str(k)], y_pred)))\n",
    "    print(\"rmse_{}= {}\".format(k, rmse))\n",
    "    return globals()['model_%s' %k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEYYpHrDw5a-"
   },
   "outputs": [],
   "source": [
    "def predict_model(k,submit, model, test_x):\n",
    "    res = model.predict(test_x)\n",
    "    submit['Y_'+k] = res\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dr9-2BnhX4Df"
   },
   "outputs": [],
   "source": [
    "model_set_cat = []\n",
    "val_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n",
    "for i in val_list:\n",
    "    model_set_cat.append(make_model_cat(i, train_x,train_y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPUiQsIK0jjb"
   },
   "outputs": [],
   "source": [
    "temp_result_cat = pd.DataFrame(res).transpose()\n",
    "temp_result_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B60JEusJfdvV"
   },
   "outputs": [],
   "source": [
    "def lg_nrmse_modified(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = mean_squared_error(gt.iloc[:,idx], preds.iloc[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt.iloc[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "        print(\"{}th's nrmse = {}\".format(idx, nrmse))\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:])\n",
    "    return score, all_nrmse\n",
    "\n",
    "score, nrmse_lis = lg_nrmse_modified(y_test, pd.DataFrame(res).transpose())\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rgCs5nWgrzV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWIjQy7diAzP"
   },
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFfSGMWeiX_5"
   },
   "source": [
    "model_02 : Best Trial: score 0.37714844628190103,\n",
    "params {'learning_rate': 0.017921025364031964, 'depth': 8, 'l2_leaf_reg': 0.14391462587307466, 'min_child_samples': 1, 'colsample_bylevel': 0.053532804311458584, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9271346787489763}\n",
    "\n",
    "model_03 : Best Trial: score 0.12376697185543123,\n",
    "params {'learning_rate': 0.03465761940957869, 'depth': 9, 'l2_leaf_reg': 0.34272110072579864, 'min_child_samples': 32, 'colsample_bylevel': 0.07225401264451321, 'bootstrap_type': 'MVS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpFAj5j6iQWg"
   },
   "outputs": [],
   "source": [
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP9bW1K07DX4"
   },
   "outputs": [],
   "source": [
    "1e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oomSBSwfd3V"
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {}\n",
    "    param['learning_rate'] = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0)\n",
    "    param['depth'] = trial.suggest_int('depth', 1, 15)\n",
    "    param['l2_leaf_reg'] = trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e1)\n",
    "    param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n",
    "    param['colsample_bylevel'] = trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1)\n",
    "    param[\"bootstrap_type\"] = trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n",
    "    param['use_best_model'] = True\n",
    "    param['eval_metric'] = 'RMSE'\n",
    "    param['cat_features'] = ['X_04','X_23','X_47','X_48']\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    regressor = catboost.CatBoostRegressor(**param)\n",
    "\n",
    "    regressor.fit(X_train, y_train, eval_set=[(X_test, y_test)],early_stopping_rounds=100, verbose = 0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, regressor.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_01'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "\n",
    "study_1 = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_1.optimize(objective, n_trials=100)\n",
    "trial_1 = study_1.best_trial\n",
    "trial_params = trial_1.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"01\",trial_1.value, trial_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5pljBZaj3jw"
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {}\n",
    "    param['learning_rate'] = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0)\n",
    "    param['depth'] = trial.suggest_int('depth', 1, 15)\n",
    "    param['l2_leaf_reg'] = trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e1)\n",
    "    param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n",
    "    param['colsample_bylevel'] = trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1)\n",
    "    param[\"bootstrap_type\"] = trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n",
    "    param['use_best_model'] = True\n",
    "    param['eval_metric'] = 'RMSE'\n",
    "    param['cat_features'] = ['X_04','X_23','X_47','X_48']\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    regressor = catboost.CatBoostRegressor(**param)\n",
    "\n",
    "    regressor.fit(X_train, y_train, eval_set=[(X_test, y_test)],early_stopping_rounds=100, verbose = 0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, regressor.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_02'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "\n",
    "study_2 = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_2.optimize(objective, n_trials=100)\n",
    "trial_2 = study_2.best_trial\n",
    "trial_params = trial_2.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"02\",trial_2.value, trial_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiDj3pa9j74H"
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {}\n",
    "    param['learning_rate'] = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0)\n",
    "    param['depth'] = trial.suggest_int('depth', 1, 15)\n",
    "    param['l2_leaf_reg'] = trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e0)\n",
    "    param['min_child_samples'] = trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32])\n",
    "    param['colsample_bylevel'] = trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1)\n",
    "    param[\"bootstrap_type\"] = trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n",
    "    param['use_best_model'] = True\n",
    "    param['eval_metric'] = 'RMSE'\n",
    "    param['cat_features'] = ['X_04','X_23','X_47','X_48']\n",
    "\n",
    "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "    regressor = catboost.CatBoostRegressor(**param)\n",
    "\n",
    "    regressor.fit(X_train, y_train, eval_set=[(X_test, y_test)],early_stopping_rounds=100, verbose = 0)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, regressor.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_03'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "\n",
    "study_3 = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_3.optimize(objective, n_trials=100)\n",
    "trial_3 = study_3.best_trial\n",
    "trial_params = trial_3.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"03\",trial_3.value, trial_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFtuyOX1iEXb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK8T5erIUTGP"
   },
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDO9xCQhu1KL"
   },
   "source": [
    "##### modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWsMtKM6UgOG"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNt37eyMZNK1"
   },
   "outputs": [],
   "source": [
    "params_lgb_dic={}\n",
    "params_lgb_dic['others'] = {'learning_rate': 0.01, \n",
    "          'max_depth': 16, \n",
    "          'boosting': 'gbdt', \n",
    "          'objective': 'regression', \n",
    "          'metric': 'mse', \n",
    "          'is_training_metric': True, \n",
    "          'num_leaves': 144, \n",
    "          'feature_fraction': 0.9, \n",
    "          'bagging_fraction': 0.7, \n",
    "          'bagging_freq': 5, \n",
    "          'seed':1422}\n",
    "params_lgb_dic['model_01'] = {'max_depth': 10, 'learning_rate': 0.003331149803117528, 'n_estimators': 2501, 'min_child_samples': 96, 'subsample': 0.47474868991879937, 'seed':1422}\n",
    "params_lgb_dic['model_02'] = {'max_depth': 15, 'learning_rate': 0.009731598902017668, 'n_estimators': 1560, 'min_child_samples': 51, 'subsample': 0.5552419580114899, 'seed':1422}\n",
    "params_lgb_dic['model_03'] = {'max_depth': 9, 'learning_rate': 0.006800685468837971, 'n_estimators': 1819, 'min_child_samples': 88, 'subsample': 0.5090800153202655, 'seed':1422}\n",
    "params_lgb_dic['model_04'] = {'max_depth': 12, 'learning_rate': 0.007937084887379823, 'n_estimators': 2999, 'min_child_samples': 70, 'subsample': 0.9300165134258743, 'seed':1422}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwynrecSUSLH"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "def make_model(k,train_x, train_y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_'+str(k)], test_size = 0.2, random_state=1422)\n",
    "    train_ds = lgb.Dataset(X_train, label = y_train) \n",
    "    test_ds = lgb.Dataset(X_test, label = y_test)\n",
    "\n",
    " \n",
    "    if k in ['01','02','03','04']:\n",
    "        params = params_lgb_dic['model_%s' %k]\n",
    "        print(\"model {} tuned_parameter used\".format(k))\n",
    "    else:\n",
    "        params = params_lgb_dic['others']\n",
    "    globals()['model_%s' %k] = lgb.LGBMRegressor(**params)\n",
    "\n",
    "\n",
    "    # globals()['model_%s' %k] = lgb.train(train_ds, test_ds, **params)\n",
    "    lgb_model = globals()['model_%s' %k].fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=25)\n",
    "   \n",
    "    y_pred = globals()['model_%s' %k].predict(X_test)\n",
    "    res.append(y_pred)\n",
    "    rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    print(\"rmse_{}= {}\".format(k, rmse))\n",
    "    return globals()['model_%s' %k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92L7WVjXUSO8"
   },
   "outputs": [],
   "source": [
    "model_set_lgb = []\n",
    "val_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n",
    "for i in val_list:\n",
    "    model_set_lgb.append(make_model(i, train_x,train_y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6weZ-X2Pjans"
   },
   "outputs": [],
   "source": [
    "temp_result_lgb =pd.DataFrame(res).transpose()\n",
    "temp_result_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mk_l6N-ml4ne"
   },
   "outputs": [],
   "source": [
    "temp_result_cat = pd.DataFrame(res).transpose()\n",
    "temp_result_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yfxv8qBjarz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state=1422)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lxCW2DAjauW"
   },
   "outputs": [],
   "source": [
    "def lg_nrmse_modified(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = mean_squared_error(gt.iloc[:,idx], preds.iloc[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt.iloc[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "        print(\"{}th's nrmse = {}\".format(idx, nrmse))\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:])\n",
    "    return score, all_nrmse\n",
    "\n",
    "score, nrmse_lis = lg_nrmse_modified(y_test,temp_result_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9xoa2cwnSHd"
   },
   "outputs": [],
   "source": [
    "z = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "St1f04Mrnd68"
   },
   "outputs": [],
   "source": [
    "score, nrmse_lis = lg_nrmse_modified(y_test,temp_result_cat*0.5+temp_result_lgb*0.5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYX4_h_fnd9u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4xkFXwCnyaN"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,temp_result_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeaWMS-fj3vm"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,temp_result_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSTDhY5Cj3yN"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,temp_result_cat*0.5+temp_result_lgb*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG51racOj300"
   },
   "outputs": [],
   "source": [
    "temp_result_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJB4MMUA2G-k"
   },
   "outputs": [],
   "source": [
    "temp_result_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y17mk2Hvz84R"
   },
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbrpZ6Kjz5m5"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbQc5ltY4Ev4"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf4suaTQz5py"
   },
   "outputs": [],
   "source": [
    "def RMSE(y, y_pred):\n",
    "    rmse = mean_squared_error(y, y_pred) ** 0.5\n",
    "    return rmse\n",
    "\n",
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        'objective': 'regression', # 회귀\n",
    "        'verbose': -1,\n",
    "        'metric': 'rmse', \n",
    "        'max_depth': trial.suggest_int('max_depth',3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    lgb_model = model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=25)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, lgb_model.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_01'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_lgb.optimize(objective, n_trials=100)\n",
    "trial = study_lgb.best_trial\n",
    "trial_params = trial.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"01\",trial.value, trial_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pog_y8JIqgcy"
   },
   "source": [
    "lgm_model_01 : Best Trial: score 0.34624666835962165,\n",
    "params {'max_depth': 10, 'learning_rate': 0.003331149803117528, 'n_estimators': 2501, 'min_child_samples': 96, 'subsample': 0.47474868991879937}\n",
    "\n",
    "model_02 : Best Trial: score 0.3772391615981788,\n",
    "params {'max_depth': 15, 'learning_rate': 0.009731598902017668, 'n_estimators': 1560, 'min_child_samples': 51, 'subsample': 0.5552419580114899}\n",
    "\n",
    "model_03 : Best Trial: score 0.35231245477354645,\n",
    "params {'max_depth': 9, 'learning_rate': 0.006800685468837971, 'n_estimators': 1819, 'min_child_samples': 88, 'subsample': 0.5090800153202655}\n",
    "\n",
    "model_04 : Best Trial: score 2.517665894515901,\n",
    "params {'max_depth': 12, 'learning_rate': 0.007937084887379823, 'n_estimators': 2999, 'min_child_samples': 70, 'subsample': 0.9300165134258743}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZQ2HygQz5sS"
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        'objective': 'regression', # 회귀\n",
    "        'verbose': -1,\n",
    "        'metric': 'rmse', \n",
    "        'max_depth': trial.suggest_int('max_depth',3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    lgb_model = model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=25)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, lgb_model.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_02'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "study_lgb2 = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_lgb2.optimize(objective, n_trials=100)\n",
    "trial = study_lgb2.best_trial\n",
    "trial_params = trial.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"02\",trial.value, trial_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWFSALigqkVF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C22LRLhWz5vh"
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        'objective': 'regression', # 회귀\n",
    "        'verbose': -1,\n",
    "        'metric': 'rmse', \n",
    "        'max_depth': trial.suggest_int('max_depth',3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    lgb_model = model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=25)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, lgb_model.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_03'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "study_lgb2 = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_lgb2.optimize(objective, n_trials=100)\n",
    "trial = study_lgb2.best_trial\n",
    "trial_params = trial.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"03\",trial.value, trial_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-gziQ-Oz5x2"
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=1422)\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    param = {\n",
    "        'objective': 'regression', # 회귀\n",
    "        'verbose': -1,\n",
    "        'metric': 'rmse', \n",
    "        'max_depth': trial.suggest_int('max_depth',3, 15),\n",
    "        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "    lgb_model = model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0, early_stopping_rounds=25)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, lgb_model.predict(X_test)))\n",
    "    return rmse\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_04'], test_size = 0.2, random_state=1422)\n",
    "\n",
    "study_lgb2 = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study_lgb2.optimize(objective, n_trials=100)\n",
    "trial = study_lgb2.best_trial\n",
    "trial_params = trial.params\n",
    "print('model_{} : Best Trial: score {},\\nparams {}'.format(\"04\",trial.value, trial_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8TevmCbz50B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Tc9m3pzUSSE"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,(pd.DataFrame(res).transpose()*0.6+temp_lgb_result*0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jt0UfVSebwO"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,(pd.DataFrame(res).transpose()*0.7+temp_lgb_result*0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJVxapPjebze"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,(pd.DataFrame(res).transpose()*0.5+temp_lgb_result*0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km_dR-7Eeb2d"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,(pd.DataFrame(res).transpose()*0.4+temp_lgb_result*0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQjNoL7veb41"
   },
   "outputs": [],
   "source": [
    "temp_cat_result = pd.DataFrame(res).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZx8UxkEeL_w"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,temp_cat_result*0.5+temp_lgb_result*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ejbe_Fsd_5Q"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,temp_lgb_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-981zJCygE7G"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,temp_cat_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w66ZkGI0USU0"
   },
   "outputs": [],
   "source": [
    "temp_lgb_result = pd.DataFrame(res).transpose()\n",
    "temp_lgb_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up1VdoZkdT-R"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state=1422)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djYm2INXV3xO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtF9HWifV3xP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zX7qgC4oV3xP",
    "outputId": "e28a637c-d556-407e-895b-d29e54d25607"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_19</th>\n",
       "      <th>X_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.11</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.97</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.04</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.05</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.04</td>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39602</th>\n",
       "      <td>3.20</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>3.15</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>3.23</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>3.18</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>3.11</td>\n",
       "      <td>1.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39607 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X_19  X_30\n",
       "0      3.11  1.49\n",
       "1      2.97  1.49\n",
       "2      3.04  1.49\n",
       "3      3.05  1.47\n",
       "4      3.04  1.49\n",
       "...     ...   ...\n",
       "39602  3.20  1.37\n",
       "39603  3.15  1.40\n",
       "39604  3.23  1.39\n",
       "39605  3.18  1.37\n",
       "39606  3.11  1.39\n",
       "\n",
       "[39607 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[['X_19','X_30']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxNh-5e3vFyn"
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LepmxmHadT5q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 1422)\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kf.split(train_x)):\n",
    "    temp_train_x = train_x.iloc[train,:]\n",
    "    temp_train_y = train_y.iloc[train,:]\n",
    "    temp_test_x = train_x.iloc[test,:]\n",
    "    temp_test_y = train_y.iloc[test,:]\n",
    "\n",
    "    temp_model_set= [make_model_val(i,temp_train_x, temp_train_y, temp_test_x,temp_test_y) for i in val_list]\n",
    "    temp_res = [temp_model_set[i].predict(temp_test_x) for i in range(0,14)]\n",
    "    score = lg_nrmse(temp_test_y,pd.DataFrame(temp_res).transpose())\n",
    "    scores.append(score)\n",
    "    print('Fold: %2d, Accuracy: %.3f' % (k+1, score))\n",
    " \n",
    "print('\\n\\nCross-Validation accuracy: %.3f +/- %.3f' %(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekWFku_pvkTv"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(train_y.iloc[test,:],pd.DataFrame(temp_res).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2BXH8cMkIQf"
   },
   "source": [
    "base - 1.9501036680150488\n",
    "\n",
    "0813_1.9480520342909105\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGp6P26CmDpv"
   },
   "source": [
    "### Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDiG4z4mdUDw"
   },
   "outputs": [],
   "source": [
    "def predict_model(k,submit, model, test_x):\n",
    "    res = model.predict(test_x)\n",
    "    submit['Y_'+k] = res\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1660654793548,
     "user": {
      "displayName": "강동인",
      "userId": "14950362428710487115"
     },
     "user_tz": -540
    },
    "id": "9bFIs4SSnXam",
    "outputId": "ed3a1adf-069f-46cf-c823-f8d4e17a3742"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>TEST_39604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>TEST_39605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>TEST_39606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>TEST_39607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39607</th>\n",
       "      <td>TEST_39608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39608 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  Y_01  Y_02  Y_03  Y_04  Y_05  Y_06  Y_07  Y_08  Y_09  Y_10  \\\n",
       "0      TEST_00001     0     0     0     0     0     0     0     0     0     0   \n",
       "1      TEST_00002     0     0     0     0     0     0     0     0     0     0   \n",
       "2      TEST_00003     0     0     0     0     0     0     0     0     0     0   \n",
       "3      TEST_00004     0     0     0     0     0     0     0     0     0     0   \n",
       "4      TEST_00005     0     0     0     0     0     0     0     0     0     0   \n",
       "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "39603  TEST_39604     0     0     0     0     0     0     0     0     0     0   \n",
       "39604  TEST_39605     0     0     0     0     0     0     0     0     0     0   \n",
       "39605  TEST_39606     0     0     0     0     0     0     0     0     0     0   \n",
       "39606  TEST_39607     0     0     0     0     0     0     0     0     0     0   \n",
       "39607  TEST_39608     0     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "       Y_11  Y_12  Y_13  Y_14  \n",
       "0         0     0     0     0  \n",
       "1         0     0     0     0  \n",
       "2         0     0     0     0  \n",
       "3         0     0     0     0  \n",
       "4         0     0     0     0  \n",
       "...     ...   ...   ...   ...  \n",
       "39603     0     0     0     0  \n",
       "39604     0     0     0     0  \n",
       "39605     0     0     0     0  \n",
       "39606     0     0     0     0  \n",
       "39607     0     0     0     0  \n",
       "\n",
       "[39608 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(base+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "s4vbOR2Rnu9Q"
   },
   "outputs": [],
   "source": [
    "submit_x = pd.read_csv(base+'test.csv')\n",
    "submit_x.drop(\"ID\",axis =1 ,inplace =True)\n",
    "submit_df = pd.read_csv(base+'sample_submission.csv')\n",
    "submit_1 = submit_df.copy()\n",
    "submit_2 = submit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "otni1BA9nu_4"
   },
   "outputs": [],
   "source": [
    "basic_col = submit_x.columns\n",
    "\n",
    "for col in basic_col:\n",
    "    if col in ['X_10','X_11','X_38','X_39',\"X_40\",\"X_45\"]:\n",
    "         continue\n",
    "    submit_x[col+\"_log\"] = submit_x[col].apply(lambda x : math.log(x))\n",
    "\n",
    "\n",
    "submit_x[['X_04','X_23','X_47','X_48']] = submit_x[['X_04','X_23','X_47','X_48']].astype(\"category\")\n",
    "for a,b in itertools.combinations(temp_1,2):\n",
    "  submit_x['abs_'+a+b] = abs(submit_x[a]-submit_x[b])\n",
    "\n",
    "for a,b in itertools.combinations(temp_2,2):\n",
    "  submit_x['abs_'+a+b] = abs(submit_x[a]-submit_x[b])\n",
    "\n",
    "for a,b in itertools.combinations(temp_3,2):\n",
    "  submit_x['abs_'+a+b] = abs(submit_x[a]-submit_x[b])\n",
    "\n",
    "submit_x['PCB_sum_1'] = submit_x['X_01']+submit_x['X_02']\n",
    "submit_x['PCB_sum_2'] = submit_x['X_05']+submit_x['X_06']\n",
    "submit_x['radi_sum_area'] = submit_x['X_07'] + submit_x['X_08']+ submit_x['X_09']\n",
    "submit_x['radi_1_weight_per_area'] = submit_x['X_03'] / submit_x['X_07']\n",
    "submit_x['radi_2_weight_per_area'] = submit_x['X_10'] / submit_x['X_08']\n",
    "submit_x['radi_3_weight_per_area'] = submit_x['X_11'] / submit_x['X_09']\n",
    "submit_x['abs_X_41X_14'] =abs(submit_x[\"X_41\"]- submit_x['X_14'])\n",
    "submit_x['abs_X_42X_15'] =abs(submit_x[\"X_42\"]- submit_x['X_15'])\n",
    "submit_x['abs_X_43X_16'] =abs(submit_x[\"X_43\"]- submit_x['X_16'])\n",
    "submit_x['abs_X_44X_17'] =abs(submit_x[\"X_44\"]- submit_x['X_17'])\n",
    "\n",
    "\n",
    "submit_x['var_antena_loc'] = submit_x[['X_14','X_15','X_16','X_17',\"X_18\"]].apply(lambda x : np.var(x), axis =1)\n",
    "submit_x['std_nthscrew_depth'] = submit_x[['X_19','X_20','X_21','X_22']].apply(lambda x : np.std(x), axis =1)\n",
    "submit_x['std_connector'] = submit_x[['X_24','X_25','X_26','X_27',\"X_28\",\"X_29\"]].apply(lambda x : np.std(x), axis =1)\n",
    "submit_x['std_screw_depth'] = submit_x[['X_30','X_31','X_32','X_33']].apply(lambda x : np.std(x), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_bYYsqow5fb"
   },
   "outputs": [],
   "source": [
    "for i in val_list:\n",
    "    predict_model(i,submit_1, model_set_cat[int(i)-1],submit_x)\n",
    "    predict_model(i,submit_2 , model_set_lgb[int(i)-1], submit_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQVLO3KDpHnB"
   },
   "outputs": [],
   "source": [
    "submit_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91e2c8SxpHpd"
   },
   "outputs": [],
   "source": [
    "submit_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXcaMIvcpZTG"
   },
   "outputs": [],
   "source": [
    "submit_df[submit_df.columns[1:]] = submit_1[submit_1.columns[1:]]*0.5 + submit_2[submit_2.columns[1:]]*0.5\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuLcgVl8pvCO"
   },
   "outputs": [],
   "source": [
    "submit_df['Y_01'] = submit_2['Y_01']\n",
    "submit_df['Y_02'] = submit_1['Y_02']*0.4+submit_2['Y_02']*0.6\n",
    "submit_df['Y_03'] = submit_1['Y_03']*0.4+submit_2['Y_03']*0.6\n",
    "submit_df['Y_04'] = submit_1['Y_04']*0.5+submit_2['Y_04']*0.5\n",
    "submit_df['Y_05'] = submit_1['Y_05']*0.6+submit_2['Y_05']*0.4\n",
    "submit_df['Y_06'] = submit_1['Y_06']\n",
    "submit_df['Y_07'] = submit_1['Y_07']\n",
    "submit_df['Y_08'] = submit_1['Y_08']\n",
    "submit_df['Y_09'] = submit_1['Y_09']\n",
    "submit_df['Y_10'] = submit_1['Y_10']\n",
    "submit_df['Y_11'] = submit_1['Y_11']\n",
    "submit_df['Y_12'] = submit_1['Y_12']\n",
    "submit_df['Y_13'] = submit_1['Y_13']*0.6+submit_2['Y_13']*0.4\n",
    "submit_df['Y_14'] = submit_1['Y_14']*0.6+submit_2['Y_14']*0.4\n",
    "submit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Tn618SpvFQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmbnHtkRpZWO"
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv(base+\"cat+lgv+fev4_0816_!.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vthms-EVZKpF"
   },
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Jffq-Zkw5pb"
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_importance = model_set_cat[0].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(train_x.columns)[sorted_idx])\n",
    "plt.title('Feature Importance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HTlAxs_T4Ci"
   },
   "outputs": [],
   "source": [
    "for i in sorted_idx:\n",
    "    print(np.array(train_x.columns)[i], feature_importance[i] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9vgsWg3T4F6"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(lis, columns = [\"x_\"+str(i) for i in range(0,56)]).to_csv(\"catboost_feature_importance_0813.csv\", index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJYORcYSbV5N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9VF8NcCvsjx"
   },
   "source": [
    "### STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZofZeTALbV90"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.linear_model import Lars, LassoLars, OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression, PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor, TheilSenRegressor, HuberRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "II0kJzHoV3xm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Pz2A61b0v5kb"
   },
   "outputs": [],
   "source": [
    "## 기존 26개 후보군에서 줄임\n",
    "\n",
    "seed = 1422\n",
    "# estimator1 = LinearRegression(n_jobs=-1)\n",
    "# estimator2 = Ridge(random_state=seed)\n",
    "# estimator3 = Lasso(random_state=seed)\n",
    "estimator1 = ElasticNet(normalize =True, random_state=seed)\n",
    "# estimator5 = LassoLars(random_state=seed)\n",
    "# estimator6 = BayesianRidge()\n",
    "# estimator7 = KernelRidge()\n",
    "estimator2 = KNeighborsRegressor(n_neighbors = 10, n_jobs=-1)\n",
    "# estimator3= DecisionTreeRegressor(random_state=seed)\n",
    "estimator3= ExtraTreeRegressor(random_state=seed)\n",
    "# estimator4= BaggingRegressor(n_jobs=-1, random_state=seed)\n",
    "estimator4= ExtraTreesRegressor(n_jobs=-1, random_state=seed)\n",
    "estimator5= RandomForestRegressor(n_jobs=-1, random_state=seed)\n",
    "estimator6= GradientBoostingRegressor(random_state=seed)\n",
    "estimator7= XGBRegressor(n_jobs=-1, random_state=seed)\n",
    "estimator8= LGBMRegressor(n_jobs=-1, random_state=seed)\n",
    "estimator9= CatBoostRegressor(verbose=False, random_state=seed)\n",
    "estimator10= MLPRegressor(random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2mv2acOv5nQ"
   },
   "outputs": [],
   "source": [
    "def get_stacking_ml_datasets(model, train_x, train_y_n, n_folds):\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    nrmse = 9999\n",
    "    best_model = model\n",
    "    scores = []\n",
    "    best_score =0\n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(train_x, train_y_n)):\n",
    "        X_tr = train_x.iloc[train_index]\n",
    "        y_tr = train_y_n.iloc[train_index]\n",
    "        X_val = train_x.iloc[valid_index]\n",
    "        y_val = train_y_n.iloc[valid_index]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "        t_nrmse = sep_lg_nrmse(y_val, y_pred)\n",
    "        scores.append(t_nrmse)\n",
    "        if t_nrmse<nrmse:\n",
    "            nrmse = t_nrmse\n",
    "            best_score= t_nrmse\n",
    "            best_model = model\n",
    "    print(best_score)\n",
    "    \n",
    "    return best_model,best_score, np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LNfJtbTV3x5"
   },
   "outputs": [],
   "source": [
    "val_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQEGAXIRv8rX",
    "outputId": "d9bdc195-4f78-4813-97ff-fed83090dd12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25919468122688283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:34, 20.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2667721037636585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:40, 13.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36897007621088507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:31, 52.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2537833779343559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:03, 109.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2534114141815342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [10:28, 162.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2524922388677299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [10:55, 118.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25930379009895504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [10:59, 81.86s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2527476444546125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [11:57, 74.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25310690856790913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [12:37, 63.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8863641033369078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [13:19, 72.68s/it]\u001b[A\n",
      "  7%|▋         | 1/14 [13:19<2:53:13, 799.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8863641033369078\n",
      "[GradientBoostingRegressor(random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3625525844041752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:34, 20.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3726206392576081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:40, 13.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5083032569229472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:35, 53.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35508911911165203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:21, 115.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35508669815806665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [10:47, 166.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35366104010193394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [11:17, 121.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3645210754297704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [11:21, 84.61s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.354147236726744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [12:21, 76.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3544476679673753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [13:01, 65.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496149936024939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [13:41, 74.70s/it]\u001b[A\n",
      " 14%|█▍        | 2/14 [27:01<2:42:30, 812.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9496149936024939\n",
      "[GradientBoostingRegressor(random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.52it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35423988827477143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:34, 20.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3649404813506122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:40, 13.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49848666311249457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:20, 47.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484495617205753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:55, 108.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3485769786169274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [10:20, 161.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3467698001632621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [10:47, 117.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35701975803223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [10:51, 81.40s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3468494656118667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [11:48, 73.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34761222431145666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [12:45, 68.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653549950983224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [13:43, 74.86s/it]\u001b[A\n",
      " 21%|██▏       | 3/14 [40:44<2:29:52, 817.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653549950983224\n",
      "[GradientBoostingRegressor(random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19195431323447365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:38, 22.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.197783694003689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:44, 15.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26685268116781236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:39, 54.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18545122538372175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:21, 114.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18520129143225167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [10:47, 166.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18667279019273483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [11:19, 122.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18994445804121834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [11:23, 84.64s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18505956460343528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [12:24, 77.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18431267129828954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [12:57, 63.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24380813679401117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [13:29, 73.55s/it]\u001b[A\n",
      " 29%|██▊       | 4/14 [54:13<2:15:41, 814.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24380813679401117\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.54it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07935992802677702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:33, 19.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08184975862623912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:39, 13.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11495375262717329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:24, 49.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0784480344583725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:13, 114.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07839480255088759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [10:39, 165.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07790484272077951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [11:06, 120.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07953216010266265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [11:10, 83.39s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07789940630518995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [12:06, 74.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0775419664601691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [12:40, 62.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08756145834427097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [13:13, 72.13s/it]\u001b[A\n",
      " 36%|███▌      | 5/14 [1:07:27<2:01:00, 806.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08756145834427097\n",
      "[GradientBoostingRegressor(random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.52it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1042122079791018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:33, 19.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10758959371499392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:39, 13.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1474598967323663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:21, 48.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09868366605215807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:36, 122.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0983415866525469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [11:01, 171.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10228981097851189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [11:28, 124.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10349952045255457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [11:31, 85.71s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09530306943559676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [12:23, 74.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09509871043811023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [12:58, 62.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12627755940399665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [13:32, 73.90s/it]\u001b[A\n",
      " 43%|████▎     | 6/14 [1:21:00<1:47:50, 808.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12627755940399665\n",
      "[ExtraTreesRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.67it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13029084292965723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:30, 18.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13517876154055378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:36, 12.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18451701461985598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:07, 43.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12856366974418545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:29, 100.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12829892184260783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [09:48, 154.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12784503097810562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [10:14, 112.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13196308814425575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [10:18, 77.77s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12813728704564162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [11:10, 69.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12785219353847388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [11:59, 63.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23330198295413432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [12:48, 69.83s/it]\u001b[A\n",
      " 50%|█████     | 7/14 [1:33:48<1:32:48, 795.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23330198295413432\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.69it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024600903565809993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:29, 17.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0251863655177599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:35, 11.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03404618954520932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:02, 41.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023438474672420134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:20, 97.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023351404971097854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [09:38, 152.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023501291606298113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [10:03, 110.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023945695461180923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [10:07, 76.68s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02339459569102139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [11:00, 69.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023356375350378326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [11:32, 57.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049120683781567476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [12:05, 65.93s/it]\u001b[A\n",
      " 57%|█████▋    | 8/14 [1:45:53<1:17:18, 773.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.049120683781567476\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.68it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02432513364158283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:30, 17.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024988234555855214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:35, 12.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03416857883269341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:03, 42.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023267724923098632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:23, 98.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0232476222737395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [09:43, 153.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023337366777486875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [10:08, 111.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023762717347022465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [10:11, 77.20s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02324050479192586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [11:05, 69.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023193341208471847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [11:36, 57.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043775379324321004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [12:07, 66.15s/it]\u001b[A\n",
      " 64%|██████▍   | 9/14 [1:58:01<1:03:14, 758.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043775379324321004\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.66it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04034466524606535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:29, 17.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04066371548988578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:35, 11.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05531667240776208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [02:05, 42.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03794800246343876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:29, 101.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03785127073732086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [09:49, 155.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03814367067776984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [10:15, 112.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038684663106807377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [10:18, 77.96s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03794301033517013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [11:13, 70.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03745223113238535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [11:51, 60.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0600499617782897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [12:22, 67.49s/it]\u001b[A\n",
      " 71%|███████▏  | 10/14 [2:10:23<50:15, 753.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0600499617782897\n",
      "[ExtraTreesRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.87it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033460181881698796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:29, 17.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03465013432509602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:33, 11.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04801285645434657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [01:25, 27.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03290506341671947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [03:05, 53.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032876629424366435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:26, 103.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03291115906235509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [06:41, 74.77s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033462136971201484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [06:44, 51.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032754261755919375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [07:27, 49.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.032622289839299816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [08:20, 50.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045158471901746815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [09:14, 50.43s/it]\u001b[A\n",
      " 79%|███████▊  | 11/14 [2:19:38<34:38, 692.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045158471901746815\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024615908162766292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:36, 21.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025241268847967735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:42, 14.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034258343709689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [01:40, 31.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023486925093249563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [03:24, 57.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02351051245202875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:43, 105.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023606660824376832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [06:58, 76.11s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024102433148485976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [07:01, 52.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023481639815206772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [07:43, 49.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023443505950680246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [08:18, 44.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03946910095357839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [08:51, 48.35s/it]\u001b[A\n",
      " 86%|████████▌ | 12/14 [2:28:30<21:27, 643.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03946910095357839\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02450519210870289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:35, 20.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.025086287856529572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:40, 13.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03439966113534506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [01:38, 30.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02338362703682494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [03:21, 56.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02336141820317805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:40, 105.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023487903317713082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [06:55, 75.79s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024000133113643414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [06:58, 52.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023394778953187558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [07:40, 49.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02335729808798057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [08:13, 44.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04826457121011723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [08:46, 47.91s/it]\u001b[A\n",
      " 93%|█████████▎| 13/14 [2:37:17<10:08, 608.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04826457121011723\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[AC:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rkd20\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), ElasticNet())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to original_alpha * np.sqrt(n_samples) if l1_ratio is 1, and to original_alpha * n_samples if l1_ratio is 0. For other values of l1_ratio, no analytic formula is available.\n",
      "  warnings.warn(\n",
      "\n",
      "1it [00:00,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02463076512271127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:35, 20.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02523855205005741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:41, 13.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03413874104122581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [01:38, 31.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023548751581005334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [03:22, 57.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023558788543392228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:42, 105.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02363662434423336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [06:57, 76.25s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024126997467606576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8it [07:00, 52.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023536763946729208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9it [07:44, 49.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023487458624650744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10it [08:20, 45.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0454704698424398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11it [08:56, 48.80s/it]\u001b[A\n",
      "100%|██████████| 14/14 [2:46:13<00:00, 712.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0454704698424398\n",
      "[RandomForestRegressor(n_jobs=-1, random_state=1422), LGBMRegressor(random_state=1422), <catboost.core.CatBoostRegressor object at 0x000001D3B0830B50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "base_ml = [\n",
    "           estimator1, estimator2, estimator3, estimator4, estimator5, estimator6, estimator7,\n",
    "           estimator8, estimator9, estimator10\n",
    "           ]\n",
    "\n",
    "val_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n",
    "final_model = []\n",
    "for i in tqdm(val_list):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_'+str(i)], test_size = 0.2, random_state=1422)\n",
    "    best_models = []\n",
    "    scores={}\n",
    "    \n",
    "    for idx, estimator in tqdm(enumerate(base_ml)):\n",
    "        temp_best_model, temp_best_score, temp_mean_score = get_stacking_ml_datasets(estimator, train_x, train_y['Y_'+str(i)], 5)\n",
    "        scores[idx] = temp_mean_score\n",
    "        best_models.append(temp_best_model)\n",
    "    \n",
    "    sorted(scores.items(), key = lambda item : item[1])\n",
    "\n",
    "    model_idx=np.array(sorted(scores.items(), key = lambda item : item[1]))[:3, 0]\n",
    "    final_model.append([value for i, value in enumerate(best_models) if i in model_idx])\n",
    "    print(final_model[int(i)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_AsbM6VGV3x7",
    "outputId": "852de965-5f49-4977-e7e5-b69c459773ae"
   },
   "outputs": [],
   "source": [
    "# 각 feature별 best 모델값들 저장\n",
    "\n",
    "final_models = [[GradientBoostingRegressor(random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [GradientBoostingRegressor(random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [GradientBoostingRegressor(random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [GradientBoostingRegressor(random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [ExtraTreesRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [ExtraTreesRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)],\\\n",
    " [RandomForestRegressor(n_jobs=-1, random_state=1422),\\\n",
    "  LGBMRegressor(random_state=1422),\\\n",
    "  CatBoostRegressor(verbose=False, random_state=seed)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t7d55RogV3x9",
    "outputId": "3509243f-33b6-436c-adb2-ffa543e85c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[GradientBoostingRegressor(random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d89160>],\n",
       " [GradientBoostingRegressor(random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d89220>],\n",
       " [GradientBoostingRegressor(random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d892e0>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d89fa0>],\n",
       " [GradientBoostingRegressor(random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8a1f0>],\n",
       " [ExtraTreesRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8a100>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8aeb0>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8afa0>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8b0d0>],\n",
       " [ExtraTreesRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8b1c0>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8b2b0>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20698d8b3a0>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20696d9d910>],\n",
       " [RandomForestRegressor(n_jobs=-1, random_state=1422),\n",
       "  LGBMRegressor(random_state=1422),\n",
       "  <catboost.core.CatBoostRegressor at 0x20696d9d220>]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0j2YICIHYKyc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:40<00:00,  1.87s/it]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x[col+\"_log\"] = submit_x[col].apply(lambda x : math.log(x))\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['PCB_sum_1'] = submit_x['X_01']+submit_x['X_02']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['PCB_sum_2'] = submit_x['X_05']+submit_x['X_06']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['radi_sum_area'] = submit_x['X_07'] + submit_x['X_08']+ submit_x['X_09']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['radi_1_weight_per_area'] = submit_x['X_03'] / submit_x['X_07']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:61: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_41X_14'] =abs(submit_x[\"X_41\"]- submit_x['X_14'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_42X_15'] =abs(submit_x[\"X_42\"]- submit_x['X_15'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_43X_16'] =abs(submit_x[\"X_43\"]- submit_x['X_16'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_12296/3748817545.py:64: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_44X_17'] =abs(submit_x[\"X_44\"]- submit_x['X_17'])\n"
     ]
    }
   ],
   "source": [
    "submit_x = pd.read_csv(base+'test.csv')\n",
    "submit_x.drop([\"ID\",\"X_10\",\"X_11\"],axis =1 ,inplace =True)\n",
    "submit_df = pd.read_csv(base+'sample_submission.csv')\n",
    "# submit_1 = submit_df.copy()\n",
    "# submit_2 = submit_df.copy()\n",
    "\n",
    "basic_col = submit_x.columns\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "#for i in range(2,44):\n",
    "for i in tqdm(range(len(submit_x.columns))):\n",
    "    current=0\n",
    "    sum_c=[]\n",
    "    z = submit_x.loc[:, submit_x.columns[i]]\n",
    "    a = []           #필터링 된 피쳐(after)\n",
    "    b = []           #필터링 전 피쳐(before)\n",
    "    my_filter = KalmanFilter(dim_x=2,dim_z=1) #create kalman filter\n",
    "    my_filter.x = np.array([[2.],[0.]])       # initial state (location and velocity)\n",
    "    my_filter.F = np.array([[1.,1.], [0.,1.]])    # state transition matrix\n",
    "    my_filter.H = np.array([[1.,0.]])    # Measurement function\n",
    "    my_filter.P *= 1000.                 # covariance matrix\n",
    "    my_filter.R = 5                      # state uncertainty\n",
    "    my_filter.Q = Q_discrete_white_noise(dim = 2,dt=.1,var=.1) # process uncertainty   \n",
    "    for k in z.values:\n",
    "        my_filter.predict()\n",
    "        my_filter.update(k)\n",
    "        # do something with the output\n",
    "        x = my_filter.x\n",
    "        a.extend(x[0])\n",
    "        b.append(k)\n",
    "    #dataset.loc[dataset.num == num, dataset.columns[i]] = a\n",
    "    sum_c=sum_c+a\n",
    "    submit_x.loc[:,'kf_X_'+str(i)]=sum_c\n",
    "\n",
    "\n",
    "for col in basic_col:\n",
    "    if col in ['X_38','X_39',\"X_40\",\"X_45\"]:\n",
    "        continue\n",
    "    submit_x[col+\"_log\"] = submit_x[col].apply(lambda x : math.log(x))\n",
    "\n",
    "\n",
    "# submit_x[['X_04','X_23','X_47','X_48']] = submit_x[['X_04','X_23','X_47','X_48']].astype(\"category\")\n",
    "for a,b in itertools.combinations(temp_1,2):\n",
    "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
    "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
    "\n",
    "for a,b in itertools.combinations(temp_2,2):\n",
    "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
    "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
    "\n",
    "\n",
    "for a,b in itertools.combinations(temp_3,2):\n",
    "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
    "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
    "\n",
    "submit_x['PCB_sum_1'] = submit_x['X_01']+submit_x['X_02']\n",
    "submit_x['PCB_sum_2'] = submit_x['X_05']+submit_x['X_06']\n",
    "submit_x['radi_sum_area'] = submit_x['X_07'] + submit_x['X_08']+ submit_x['X_09']\n",
    "submit_x['radi_1_weight_per_area'] = submit_x['X_03'] / submit_x['X_07']\n",
    "submit_x['abs_X_41X_14'] =abs(submit_x[\"X_41\"]- submit_x['X_14'])\n",
    "submit_x['abs_X_42X_15'] =abs(submit_x[\"X_42\"]- submit_x['X_15'])\n",
    "submit_x['abs_X_43X_16'] =abs(submit_x[\"X_43\"]- submit_x['X_16'])\n",
    "submit_x['abs_X_44X_17'] =abs(submit_x[\"X_44\"]- submit_x['X_17'])\n",
    "\n",
    "\n",
    "submit_x['var_antena_loc'] = submit_x[['X_14','X_15','X_16','X_17',\"X_18\"]].apply(lambda x : np.var(x), axis =1)\n",
    "submit_x['std_nthscrew_depth'] = submit_x[['X_19','X_20','X_21','X_22']].apply(lambda x : np.std(x), axis =1)\n",
    "submit_x['std_connector'] = submit_x[['X_24','X_25','X_26','X_27',\"X_28\",\"X_29\"]].apply(lambda x : np.std(x), axis =1)\n",
    "submit_x['std_screw_depth'] = submit_x[['X_30','X_31','X_32','X_33']].apply(lambda x : np.std(x), axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    \n",
    "    rmse = mean_squared_error(gt, preds, squared=False)\n",
    "    nrmse = rmse/np.mean(np.abs(gt))\n",
    "#     print(\"nrmse = {}\".format(nrmse))\n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gSNmoTSAV3x-",
    "outputId": "7a7ad03f-e06e-40ee-d3d0-5fb7e813d090"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 01, folder counter : 0, nrmse : 0.25654464163368834\n",
      "val : 01, folder counter : 1, nrmse : 0.2579057665619735\n",
      "val : 01, folder counter : 2, nrmse : 0.2568016934532296\n",
      "val : 01, folder counter : 3, nrmse : 0.2542892076176439\n",
      "val : 01, folder counter : 4, nrmse : 0.25086343639249065\n",
      "best nrmse =  0.25086343639249065\n",
      "val : 01, folder counter : 0, nrmse : 0.2555193493431117\n",
      "val : 01, folder counter : 1, nrmse : 0.25648736391969335\n",
      "val : 01, folder counter : 2, nrmse : 0.2554669514033084\n",
      "val : 01, folder counter : 3, nrmse : 0.25357502202982685\n",
      "val : 01, folder counter : 4, nrmse : 0.25083689322979613\n",
      "best nrmse =  0.25083689322979613\n",
      "val : 01, folder counter : 0, nrmse : 0.2551929718715468\n",
      "val : 01, folder counter : 1, nrmse : 0.2564344155312914\n",
      "val : 01, folder counter : 2, nrmse : 0.25504443991564457\n",
      "val : 01, folder counter : 3, nrmse : 0.2533687097668253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/14 [10:00<2:10:02, 600.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 01, folder counter : 4, nrmse : 0.2500765375161437\n",
      "best nrmse =  0.2500765375161437\n",
      "val : 02, folder counter : 0, nrmse : 0.35814806549188005\n",
      "val : 02, folder counter : 1, nrmse : 0.3577498598205605\n",
      "val : 02, folder counter : 2, nrmse : 0.35636544913213497\n",
      "val : 02, folder counter : 3, nrmse : 0.3548717461948403\n",
      "val : 02, folder counter : 4, nrmse : 0.3521116590021726\n",
      "best nrmse =  0.3521116590021726\n",
      "val : 02, folder counter : 0, nrmse : 0.356927019264353\n",
      "val : 02, folder counter : 1, nrmse : 0.35739350768314676\n",
      "val : 02, folder counter : 2, nrmse : 0.35635012682623113\n",
      "val : 02, folder counter : 3, nrmse : 0.35393529092796994\n",
      "val : 02, folder counter : 4, nrmse : 0.3518659389688623\n",
      "best nrmse =  0.3518659389688623\n",
      "val : 02, folder counter : 0, nrmse : 0.3567055875165064\n",
      "val : 02, folder counter : 1, nrmse : 0.3567702677698721\n",
      "val : 02, folder counter : 2, nrmse : 0.3558431761919596\n",
      "val : 02, folder counter : 3, nrmse : 0.353706618340493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 2/14 [19:43<1:58:00, 590.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 02, folder counter : 4, nrmse : 0.35100132344104645\n",
      "best nrmse =  0.35100132344104645\n",
      "val : 03, folder counter : 0, nrmse : 0.34936799973580607\n",
      "val : 03, folder counter : 1, nrmse : 0.34956121891003356\n",
      "val : 03, folder counter : 2, nrmse : 0.3507462275866676\n",
      "val : 03, folder counter : 3, nrmse : 0.34803511781556656\n",
      "val : 03, folder counter : 4, nrmse : 0.345454308105839\n",
      "best nrmse =  0.345454308105839\n",
      "val : 03, folder counter : 0, nrmse : 0.3485036102250221\n",
      "val : 03, folder counter : 1, nrmse : 0.34819729339342176\n",
      "val : 03, folder counter : 2, nrmse : 0.34994544782958326\n",
      "val : 03, folder counter : 3, nrmse : 0.3469004888914431\n",
      "val : 03, folder counter : 4, nrmse : 0.34505002238958427\n",
      "best nrmse =  0.34505002238958427\n",
      "val : 03, folder counter : 0, nrmse : 0.3482253531725124\n",
      "val : 03, folder counter : 1, nrmse : 0.34819118797658705\n",
      "val : 03, folder counter : 2, nrmse : 0.349529093488935\n",
      "val : 03, folder counter : 3, nrmse : 0.3464570753197941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 3/14 [29:23<1:47:21, 585.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 03, folder counter : 4, nrmse : 0.34456871272929945\n",
      "best nrmse =  0.34456871272929945\n",
      "val : 04, folder counter : 0, nrmse : 0.17900343350009762\n",
      "val : 04, folder counter : 1, nrmse : 0.18278654942172046\n",
      "val : 04, folder counter : 2, nrmse : 0.19417144656496038\n",
      "val : 04, folder counter : 3, nrmse : 0.18207104934155738\n",
      "val : 04, folder counter : 4, nrmse : 0.17813644971394796\n",
      "best nrmse =  0.17813644971394796\n",
      "val : 04, folder counter : 0, nrmse : 0.18075918868380675\n",
      "val : 04, folder counter : 1, nrmse : 0.18600999583900726\n",
      "val : 04, folder counter : 2, nrmse : 0.1966713269768134\n",
      "val : 04, folder counter : 3, nrmse : 0.18504268481687133\n",
      "val : 04, folder counter : 4, nrmse : 0.18065578470175844\n",
      "best nrmse =  0.18065578470175844\n",
      "val : 04, folder counter : 0, nrmse : 0.17863972608728126\n",
      "val : 04, folder counter : 1, nrmse : 0.1833722038675194\n",
      "val : 04, folder counter : 2, nrmse : 0.19441551175283142\n",
      "val : 04, folder counter : 3, nrmse : 0.18308126768632674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 4/14 [35:16<1:22:18, 493.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 04, folder counter : 4, nrmse : 0.1787428909809545\n",
      "best nrmse =  0.17863972608728126\n",
      "val : 05, folder counter : 0, nrmse : 0.07729266656737757\n",
      "val : 05, folder counter : 1, nrmse : 0.07955207721692066\n",
      "val : 05, folder counter : 2, nrmse : 0.07966186433535738\n",
      "val : 05, folder counter : 3, nrmse : 0.0800052347290231\n",
      "val : 05, folder counter : 4, nrmse : 0.07882546842486518\n",
      "best nrmse =  0.07729266656737757\n",
      "val : 05, folder counter : 0, nrmse : 0.07671167675713512\n",
      "val : 05, folder counter : 1, nrmse : 0.07898886006304896\n",
      "val : 05, folder counter : 2, nrmse : 0.0788842050821845\n",
      "val : 05, folder counter : 3, nrmse : 0.07883883374833127\n",
      "val : 05, folder counter : 4, nrmse : 0.07785435912941982\n",
      "best nrmse =  0.07671167675713512\n",
      "val : 05, folder counter : 0, nrmse : 0.07602659148835428\n",
      "val : 05, folder counter : 1, nrmse : 0.07806017995238845\n",
      "val : 05, folder counter : 2, nrmse : 0.07793540787241131\n",
      "val : 05, folder counter : 3, nrmse : 0.07809934624273202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 5/14 [44:59<1:18:52, 525.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 05, folder counter : 4, nrmse : 0.07747282204821594\n",
      "best nrmse =  0.07602659148835428\n",
      "val : 06, folder counter : 0, nrmse : 0.10273054903079736\n",
      "val : 06, folder counter : 1, nrmse : 0.10409009967019466\n",
      "val : 06, folder counter : 2, nrmse : 0.10872101884606258\n",
      "val : 06, folder counter : 3, nrmse : 0.09182079693267918\n",
      "val : 06, folder counter : 4, nrmse : 0.09532237158673053\n",
      "best nrmse =  0.09182079693267918\n",
      "val : 06, folder counter : 0, nrmse : 0.09587459595955068\n",
      "val : 06, folder counter : 1, nrmse : 0.1001298542018221\n",
      "val : 06, folder counter : 2, nrmse : 0.0994558060096163\n",
      "val : 06, folder counter : 3, nrmse : 0.08600491149359424\n",
      "val : 06, folder counter : 4, nrmse : 0.10032889032092344\n",
      "best nrmse =  0.08600491149359424\n",
      "val : 06, folder counter : 0, nrmse : 0.09899213629673441\n",
      "val : 06, folder counter : 1, nrmse : 0.10131965152791296\n",
      "val : 06, folder counter : 2, nrmse : 0.10237860247242483\n",
      "val : 06, folder counter : 3, nrmse : 0.08645678966385761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 6/14 [48:15<55:09, 413.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 06, folder counter : 4, nrmse : 0.0962083287085227\n",
      "best nrmse =  0.08645678966385761\n",
      "val : 07, folder counter : 0, nrmse : 0.12841220759588776\n",
      "val : 07, folder counter : 1, nrmse : 0.12730354673017463\n",
      "val : 07, folder counter : 2, nrmse : 0.1275736944599504\n",
      "val : 07, folder counter : 3, nrmse : 0.12880636600391332\n",
      "val : 07, folder counter : 4, nrmse : 0.12625580810072182\n",
      "best nrmse =  0.12625580810072182\n",
      "val : 07, folder counter : 0, nrmse : 0.12894604773030677\n",
      "val : 07, folder counter : 1, nrmse : 0.1274051553027694\n",
      "val : 07, folder counter : 2, nrmse : 0.1278797471295188\n",
      "val : 07, folder counter : 3, nrmse : 0.12894004330713937\n",
      "val : 07, folder counter : 4, nrmse : 0.12651896627351047\n",
      "best nrmse =  0.12651896627351047\n",
      "val : 07, folder counter : 0, nrmse : 0.12827958323850536\n",
      "val : 07, folder counter : 1, nrmse : 0.12726901200711013\n",
      "val : 07, folder counter : 2, nrmse : 0.12763167579028734\n",
      "val : 07, folder counter : 3, nrmse : 0.12839337737469053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 7/14 [54:25<46:37, 399.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 07, folder counter : 4, nrmse : 0.1262533860440671\n",
      "best nrmse =  0.1262533860440671\n",
      "val : 08, folder counter : 0, nrmse : 0.022717226487137972\n",
      "val : 08, folder counter : 1, nrmse : 0.02321710753815337\n",
      "val : 08, folder counter : 2, nrmse : 0.02334456654931569\n",
      "val : 08, folder counter : 3, nrmse : 0.023670637876093317\n",
      "val : 08, folder counter : 4, nrmse : 0.023012290534647102\n",
      "best nrmse =  0.022717226487137972\n",
      "val : 08, folder counter : 0, nrmse : 0.022964704321578593\n",
      "val : 08, folder counter : 1, nrmse : 0.0234283149038456\n",
      "val : 08, folder counter : 2, nrmse : 0.023616468264499882\n",
      "val : 08, folder counter : 3, nrmse : 0.023904900478934107\n",
      "val : 08, folder counter : 4, nrmse : 0.023216315153400852\n",
      "best nrmse =  0.022964704321578593\n",
      "val : 08, folder counter : 0, nrmse : 0.022792000034323977\n",
      "val : 08, folder counter : 1, nrmse : 0.02326034253414662\n",
      "val : 08, folder counter : 2, nrmse : 0.0234055807525321\n",
      "val : 08, folder counter : 3, nrmse : 0.023649387398559114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 8/14 [1:00:34<38:58, 389.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 08, folder counter : 4, nrmse : 0.023126023642783954\n",
      "best nrmse =  0.022792000034323977\n",
      "val : 09, folder counter : 0, nrmse : 0.02255087585448065\n",
      "val : 09, folder counter : 1, nrmse : 0.023074938004255828\n",
      "val : 09, folder counter : 2, nrmse : 0.023146724196139574\n",
      "val : 09, folder counter : 3, nrmse : 0.023365673805345287\n",
      "val : 09, folder counter : 4, nrmse : 0.022900679795988116\n",
      "best nrmse =  0.02255087585448065\n",
      "val : 09, folder counter : 0, nrmse : 0.022790596397877873\n",
      "val : 09, folder counter : 1, nrmse : 0.023291977075822735\n",
      "val : 09, folder counter : 2, nrmse : 0.023398039734372492\n",
      "val : 09, folder counter : 3, nrmse : 0.023599166291202144\n",
      "val : 09, folder counter : 4, nrmse : 0.02315096675134159\n",
      "best nrmse =  0.022790596397877873\n",
      "val : 09, folder counter : 0, nrmse : 0.02259180775755439\n",
      "val : 09, folder counter : 1, nrmse : 0.02305940170271012\n",
      "val : 09, folder counter : 2, nrmse : 0.023149913068615\n",
      "val : 09, folder counter : 3, nrmse : 0.023350675691532513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 9/14 [1:06:56<32:17, 387.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 09, folder counter : 4, nrmse : 0.023001575542372858\n",
      "best nrmse =  0.02259180775755439\n",
      "val : 10, folder counter : 0, nrmse : 0.03714678228029384\n",
      "val : 10, folder counter : 1, nrmse : 0.03793520708703856\n",
      "val : 10, folder counter : 2, nrmse : 0.038015852884524824\n",
      "val : 10, folder counter : 3, nrmse : 0.0370842534291696\n",
      "val : 10, folder counter : 4, nrmse : 0.03642241743786566\n",
      "best nrmse =  0.03642241743786566\n",
      "val : 10, folder counter : 0, nrmse : 0.03704935741333621\n",
      "val : 10, folder counter : 1, nrmse : 0.03748166220171331\n",
      "val : 10, folder counter : 2, nrmse : 0.03764090717467787\n",
      "val : 10, folder counter : 3, nrmse : 0.036794144964170473\n",
      "val : 10, folder counter : 4, nrmse : 0.03658164878156289\n",
      "best nrmse =  0.03658164878156289\n",
      "val : 10, folder counter : 0, nrmse : 0.03665622260704345\n",
      "val : 10, folder counter : 1, nrmse : 0.03700733915320802\n",
      "val : 10, folder counter : 2, nrmse : 0.03747009482929434\n",
      "val : 10, folder counter : 3, nrmse : 0.036306831588450245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 10/14 [1:12:49<25:06, 376.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 10, folder counter : 4, nrmse : 0.03586965574040417\n",
      "best nrmse =  0.03586965574040417\n",
      "val : 11, folder counter : 0, nrmse : 0.031902993222055545\n",
      "val : 11, folder counter : 1, nrmse : 0.03235837586840042\n",
      "val : 11, folder counter : 2, nrmse : 0.03269621098428122\n",
      "val : 11, folder counter : 3, nrmse : 0.03311535270218466\n",
      "val : 11, folder counter : 4, nrmse : 0.03189472758214546\n",
      "best nrmse =  0.03189472758214546\n",
      "val : 11, folder counter : 0, nrmse : 0.032195131411591756\n",
      "val : 11, folder counter : 1, nrmse : 0.03283427344605284\n",
      "val : 11, folder counter : 2, nrmse : 0.032941421413687694\n",
      "val : 11, folder counter : 3, nrmse : 0.033267941233842184\n",
      "val : 11, folder counter : 4, nrmse : 0.03214357003813258\n",
      "best nrmse =  0.03214357003813258\n",
      "val : 11, folder counter : 0, nrmse : 0.031863556290535625\n",
      "val : 11, folder counter : 1, nrmse : 0.032363193302375234\n",
      "val : 11, folder counter : 2, nrmse : 0.03264791067769382\n",
      "val : 11, folder counter : 3, nrmse : 0.032977970312151585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████▊  | 11/14 [1:22:26<21:53, 437.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 11, folder counter : 4, nrmse : 0.03191271360179867\n",
      "best nrmse =  0.031863556290535625\n",
      "val : 12, folder counter : 0, nrmse : 0.02275287713463855\n",
      "val : 12, folder counter : 1, nrmse : 0.02318780498814997\n",
      "val : 12, folder counter : 2, nrmse : 0.02322003119633158\n",
      "val : 12, folder counter : 3, nrmse : 0.023419387109892175\n",
      "val : 12, folder counter : 4, nrmse : 0.02291504424258398\n",
      "best nrmse =  0.02275287713463855\n",
      "val : 12, folder counter : 0, nrmse : 0.02308766294138546\n",
      "val : 12, folder counter : 1, nrmse : 0.02339637158306441\n",
      "val : 12, folder counter : 2, nrmse : 0.023594407271723836\n",
      "val : 12, folder counter : 3, nrmse : 0.023767867681147622\n",
      "val : 12, folder counter : 4, nrmse : 0.023112413543867765\n",
      "best nrmse =  0.02308766294138546\n",
      "val : 12, folder counter : 0, nrmse : 0.022911399029420833\n",
      "val : 12, folder counter : 1, nrmse : 0.023134903572591473\n",
      "val : 12, folder counter : 2, nrmse : 0.023295967997640773\n",
      "val : 12, folder counter : 3, nrmse : 0.023510985831902628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 12/14 [1:31:47<15:51, 475.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 12, folder counter : 4, nrmse : 0.022956636162326786\n",
      "best nrmse =  0.022911399029420833\n",
      "val : 13, folder counter : 0, nrmse : 0.02263230367465612\n",
      "val : 13, folder counter : 1, nrmse : 0.023109122261976645\n",
      "val : 13, folder counter : 2, nrmse : 0.023280854687456262\n",
      "val : 13, folder counter : 3, nrmse : 0.023454579496148247\n",
      "val : 13, folder counter : 4, nrmse : 0.02277928424342889\n",
      "best nrmse =  0.02263230367465612\n",
      "val : 13, folder counter : 0, nrmse : 0.02288251397173232\n",
      "val : 13, folder counter : 1, nrmse : 0.023317979473495527\n",
      "val : 13, folder counter : 2, nrmse : 0.023511671232170638\n",
      "val : 13, folder counter : 3, nrmse : 0.02371399191109991\n",
      "val : 13, folder counter : 4, nrmse : 0.023018067637215946\n",
      "best nrmse =  0.02288251397173232\n",
      "val : 13, folder counter : 0, nrmse : 0.022738961651708973\n",
      "val : 13, folder counter : 1, nrmse : 0.02305529084415178\n",
      "val : 13, folder counter : 2, nrmse : 0.023366071483536803\n",
      "val : 13, folder counter : 3, nrmse : 0.023513125592622055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 13/14 [1:40:14<08:04, 484.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 13, folder counter : 4, nrmse : 0.02282370118693043\n",
      "best nrmse =  0.022738961651708973\n",
      "val : 14, folder counter : 0, nrmse : 0.022751063233328057\n",
      "val : 14, folder counter : 1, nrmse : 0.023084832549332485\n",
      "val : 14, folder counter : 2, nrmse : 0.02321936326976082\n",
      "val : 14, folder counter : 3, nrmse : 0.02344065510173674\n",
      "val : 14, folder counter : 4, nrmse : 0.023024083516349228\n",
      "best nrmse =  0.022751063233328057\n",
      "val : 14, folder counter : 0, nrmse : 0.02299504384258013\n",
      "val : 14, folder counter : 1, nrmse : 0.023334480972168618\n",
      "val : 14, folder counter : 2, nrmse : 0.023571586074616704\n",
      "val : 14, folder counter : 3, nrmse : 0.02372806822189466\n",
      "val : 14, folder counter : 4, nrmse : 0.023196496375876664\n",
      "best nrmse =  0.02299504384258013\n",
      "val : 14, folder counter : 0, nrmse : 0.02288683438193652\n",
      "val : 14, folder counter : 1, nrmse : 0.023037239894124366\n",
      "val : 14, folder counter : 2, nrmse : 0.023316629742618694\n",
      "val : 14, folder counter : 3, nrmse : 0.023443131485217008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [1:47:56<00:00, 462.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val : 14, folder counter : 4, nrmse : 0.023000469143782728\n",
      "best nrmse =  0.02288683438193652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#학습과 동시에 submit 예측 -> 추후 수정예정\n",
    "\n",
    "val_list = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14']\n",
    "will_use_model = []\n",
    "will_use_pred = []\n",
    "will_use_fold = []\n",
    "submit_pred = []\n",
    "for i in tqdm(val_list):\n",
    "    train_y_n = train_y[\"Y_\"+str(i)]\n",
    "    best_pred_modelwise = []\n",
    "    best_preds = []\n",
    "    best_submits = []\n",
    "    best_fold = []\n",
    "    for model in final_models[int(i)-1]:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        \n",
    "        nrmse = 9999\n",
    "        best_model = model\n",
    "        temp_pred = 0\n",
    "        temp_fold = 0\n",
    "        for folder_counter, (train_index, valid_index) in enumerate(kf.split(train_x, train_y_n)):\n",
    "            X_tr = train_x.iloc[train_index]\n",
    "            y_tr = train_y_n.iloc[train_index]\n",
    "            X_val = train_x.iloc[valid_index]\n",
    "            y_val = train_y_n.iloc[valid_index]\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_val)\n",
    "            t_nrmse = sep_lg_nrmse(y_val, y_pred)\n",
    "            print(\"val : {}, folder counter : {}, nrmse : {}\".format(i,folder_counter, t_nrmse))\n",
    "            if t_nrmse<nrmse:\n",
    "                nrmse = t_nrmse\n",
    "                best_model = model\n",
    "                temp_pred = y_pred\n",
    "                temp_fold = folder_counter\n",
    "        submit_n = best_model.predict(submit_x)\n",
    "        best_submits.append(submit_n)\n",
    "        best_pred_modelwise.append(best_model)\n",
    "        best_preds.append(temp_pred)\n",
    "        best_fold.append(temp_fold)\n",
    "        print(\"best nrmse = \",nrmse)\n",
    "    will_use_model.append(best_pred_modelwise)\n",
    "    will_use_pred.append(best_preds)\n",
    "    will_use_fold.append(best_fold)\n",
    "    submit_pred.append(best_submits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=1422, shuffle=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "kf.split(train_x, train_y)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5192/3838558189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'kf' is not defined"
     ]
    }
   ],
   "source": [
    "for a, i in enumerate(kf.split(train_x, train_y)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5192/1717386120.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "kf.split(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 4, 4],\n",
       " [4, 4, 4],\n",
       " [4, 4, 4],\n",
       " [4, 4, 0],\n",
       " [0, 0, 0],\n",
       " [3, 3, 3],\n",
       " [4, 4, 4],\n",
       " [0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [4, 4, 4],\n",
       " [4, 4, 0],\n",
       " [0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [0, 0, 0]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "will_use_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([1.23451219, 1.32575294, 1.32482432, ..., 1.28248822, 1.23707826,\n",
       "         1.26394042]),\n",
       "  array([1.30005945, 1.3508053 , 1.23171535, ..., 1.28724428, 1.17464241,\n",
       "         1.22356854]),\n",
       "  array([1.34500282, 1.33549404, 1.42930778, ..., 1.33695023, 1.19768294,\n",
       "         1.29657866])],\n",
       " [array([1.00866138, 1.18166094, 1.05987867, ..., 0.96175746, 0.93838102,\n",
       "         0.966381  ]),\n",
       "  array([1.07453545, 1.18318218, 1.17397643, ..., 0.90354857, 0.89416027,\n",
       "         0.96110577]),\n",
       "  array([1.03324618, 0.96275728, 1.24469815, ..., 0.99682418, 0.88109769,\n",
       "         0.89033213])],\n",
       " [array([0.92442042, 1.04100727, 0.95785836, ..., 0.91553619, 0.94106719,\n",
       "         0.95769446]),\n",
       "  array([0.91263381, 0.9892922 , 0.90258794, ..., 1.00506536, 0.92488288,\n",
       "         0.99639682]),\n",
       "  array([1.10903379, 0.98496076, 0.99917495, ..., 1.10055979, 0.97218079,\n",
       "         0.98521532])],\n",
       " [array([14.04222, 14.43561, 14.7005 , ..., 13.38099, 13.3107 , 12.38587]),\n",
       "  array([15.58078512, 15.23892864, 15.19613816, ..., 13.47999824,\n",
       "         13.35628168, 13.21275643]),\n",
       "  array([13.6573483 , 14.80306019, 14.55144507, ..., 13.31523814,\n",
       "         13.16414788, 12.66894533])],\n",
       " [array([32.43958653, 32.12458118, 31.70457039, ..., 31.4606047 ,\n",
       "         31.25605795, 31.05418317]),\n",
       "  array([31.67511775, 32.04634091, 31.71938345, ..., 31.43928852,\n",
       "         30.89953144, 30.49817523]),\n",
       "  array([31.73436924, 32.27567022, 31.19138028, ..., 30.87561899,\n",
       "         30.63417859, 30.08974736])],\n",
       " [array([16.61409, 16.78445, 16.87884, ..., 16.78089, 16.71081, 16.57713]),\n",
       "  array([17.01253496, 16.79236488, 16.42394929, ..., 16.72999604,\n",
       "         16.59286709, 16.76947105]),\n",
       "  array([16.63017583, 16.61135911, 16.81673337, ..., 16.67757097,\n",
       "         16.56319332, 16.87117485])],\n",
       " [array([3.1968 , 3.03894, 3.04855, ..., 3.13251, 3.12633, 3.22974]),\n",
       "  array([3.17657052, 3.01024128, 3.10899524, ..., 3.10987005, 3.14002956,\n",
       "         3.18521657]),\n",
       "  array([3.02592415, 2.87424263, 2.88282104, ..., 3.08445778, 3.10062067,\n",
       "         3.22841578])],\n",
       " [array([-25.8019 , -25.70238, -25.92132, ..., -26.54462, -26.5554 ,\n",
       "         -26.53739]),\n",
       "  array([-25.85567811, -25.61823056, -25.81577201, ..., -26.40600826,\n",
       "         -26.56111953, -26.38142494]),\n",
       "  array([-25.84380635, -25.76940479, -25.72876301, ..., -26.5987095 ,\n",
       "         -26.57249445, -26.41870959])],\n",
       " [array([-26.05537, -25.74839, -26.02439, ..., -26.52111, -26.45422,\n",
       "         -26.48821]),\n",
       "  array([-25.76135049, -25.67353225, -25.78947149, ..., -26.48271323,\n",
       "         -26.60104577, -26.34025224]),\n",
       "  array([-25.70220296, -25.70189549, -25.85659519, ..., -26.53705152,\n",
       "         -26.43919844, -26.30466303])],\n",
       " [array([-22.49859, -21.90195, -22.33045, ..., -22.57823, -22.91522,\n",
       "         -22.7084 ]),\n",
       "  array([-22.72713437, -21.92992647, -22.35654962, ..., -22.60536501,\n",
       "         -22.69762676, -22.68034766]),\n",
       "  array([-22.03015491, -21.99659337, -22.18394161, ..., -22.3465241 ,\n",
       "         -22.79393634, -22.63438931])],\n",
       " [array([24.16446, 24.21342, 24.25138, ..., 24.33935, 24.48877, 24.3844 ]),\n",
       "  array([24.553404  , 24.73461984, 24.67352913, ..., 24.70724154,\n",
       "         24.29774589, 24.38161301]),\n",
       "  array([24.67495828, 24.73280321, 24.4908433 , ..., 24.23267589,\n",
       "         24.25975493, 24.41123526])],\n",
       " [array([-25.82895, -25.81245, -25.9608 , ..., -26.45433, -26.3354 ,\n",
       "         -26.40329]),\n",
       "  array([-25.69836677, -25.55947306, -25.79349334, ..., -26.35085624,\n",
       "         -26.3717568 , -26.44512628]),\n",
       "  array([-25.61993446, -25.53684051, -25.60035361, ..., -26.45100045,\n",
       "         -26.48264406, -26.40651974])],\n",
       " [array([-26.01877, -25.68212, -25.89342, ..., -26.49232, -26.53004,\n",
       "         -26.47946]),\n",
       "  array([-25.69815772, -25.60534221, -25.83027959, ..., -26.44720984,\n",
       "         -26.37781608, -26.36024397]),\n",
       "  array([-25.82069771, -25.50330071, -25.70403484, ..., -26.52381046,\n",
       "         -26.40216492, -26.37896983])],\n",
       " [array([-26.08361, -25.64385, -25.8503 , ..., -26.4941 , -26.46511,\n",
       "         -26.50554]),\n",
       "  array([-25.68176068, -25.42556943, -25.68941145, ..., -26.41936677,\n",
       "         -26.43084332, -26.34212071]),\n",
       "  array([-25.76664556, -25.66154282, -25.56081722, ..., -26.46065685,\n",
       "         -26.37470256, -26.39243634])]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "will_use_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrVaErDWV3x_",
    "outputId": "44068b92-43a0-4338-ad2b-284bf2b6c25a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>1.411203</td>\n",
       "      <td>1.270231</td>\n",
       "      <td>1.157351</td>\n",
       "      <td>12.729968</td>\n",
       "      <td>30.218679</td>\n",
       "      <td>15.317764</td>\n",
       "      <td>3.319986</td>\n",
       "      <td>-26.269786</td>\n",
       "      <td>-26.145384</td>\n",
       "      <td>-22.754328</td>\n",
       "      <td>24.207532</td>\n",
       "      <td>-26.150464</td>\n",
       "      <td>-26.176530</td>\n",
       "      <td>-26.158899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>1.369738</td>\n",
       "      <td>1.225033</td>\n",
       "      <td>1.159070</td>\n",
       "      <td>13.507953</td>\n",
       "      <td>30.245127</td>\n",
       "      <td>16.109916</td>\n",
       "      <td>3.252141</td>\n",
       "      <td>-26.293444</td>\n",
       "      <td>-26.494397</td>\n",
       "      <td>-22.507339</td>\n",
       "      <td>24.146070</td>\n",
       "      <td>-26.266648</td>\n",
       "      <td>-26.331711</td>\n",
       "      <td>-26.237712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>1.398354</td>\n",
       "      <td>1.085939</td>\n",
       "      <td>1.102530</td>\n",
       "      <td>15.891426</td>\n",
       "      <td>32.272181</td>\n",
       "      <td>16.139362</td>\n",
       "      <td>3.122304</td>\n",
       "      <td>-26.108558</td>\n",
       "      <td>-26.166355</td>\n",
       "      <td>-22.625140</td>\n",
       "      <td>24.391475</td>\n",
       "      <td>-26.045497</td>\n",
       "      <td>-26.080950</td>\n",
       "      <td>-26.145889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>1.331356</td>\n",
       "      <td>1.033716</td>\n",
       "      <td>1.011200</td>\n",
       "      <td>14.956081</td>\n",
       "      <td>32.307154</td>\n",
       "      <td>16.802362</td>\n",
       "      <td>3.272608</td>\n",
       "      <td>-25.862041</td>\n",
       "      <td>-25.904944</td>\n",
       "      <td>-21.993169</td>\n",
       "      <td>24.631787</td>\n",
       "      <td>-25.865593</td>\n",
       "      <td>-25.891911</td>\n",
       "      <td>-25.745877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00005</td>\n",
       "      <td>1.282550</td>\n",
       "      <td>1.038509</td>\n",
       "      <td>0.941753</td>\n",
       "      <td>14.411838</td>\n",
       "      <td>30.911137</td>\n",
       "      <td>16.733598</td>\n",
       "      <td>3.147980</td>\n",
       "      <td>-25.864387</td>\n",
       "      <td>-26.010221</td>\n",
       "      <td>-22.582241</td>\n",
       "      <td>24.451207</td>\n",
       "      <td>-25.655802</td>\n",
       "      <td>-25.848202</td>\n",
       "      <td>-25.847849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>TEST_39604</td>\n",
       "      <td>1.281709</td>\n",
       "      <td>0.938325</td>\n",
       "      <td>1.007118</td>\n",
       "      <td>13.135560</td>\n",
       "      <td>30.885437</td>\n",
       "      <td>16.648009</td>\n",
       "      <td>3.144713</td>\n",
       "      <td>-26.524722</td>\n",
       "      <td>-26.516356</td>\n",
       "      <td>-22.974088</td>\n",
       "      <td>24.275399</td>\n",
       "      <td>-26.429750</td>\n",
       "      <td>-26.435848</td>\n",
       "      <td>-26.414641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>TEST_39605</td>\n",
       "      <td>1.216538</td>\n",
       "      <td>0.909782</td>\n",
       "      <td>0.950816</td>\n",
       "      <td>14.245863</td>\n",
       "      <td>31.388069</td>\n",
       "      <td>16.732544</td>\n",
       "      <td>3.134051</td>\n",
       "      <td>-26.495966</td>\n",
       "      <td>-26.394527</td>\n",
       "      <td>-22.745000</td>\n",
       "      <td>24.349661</td>\n",
       "      <td>-26.475413</td>\n",
       "      <td>-26.372099</td>\n",
       "      <td>-26.341783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>TEST_39606</td>\n",
       "      <td>1.249731</td>\n",
       "      <td>0.921726</td>\n",
       "      <td>0.935355</td>\n",
       "      <td>13.170882</td>\n",
       "      <td>31.174406</td>\n",
       "      <td>16.668403</td>\n",
       "      <td>3.165195</td>\n",
       "      <td>-26.514139</td>\n",
       "      <td>-26.424853</td>\n",
       "      <td>-22.873743</td>\n",
       "      <td>24.359275</td>\n",
       "      <td>-26.422926</td>\n",
       "      <td>-26.412130</td>\n",
       "      <td>-26.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>TEST_39607</td>\n",
       "      <td>1.207937</td>\n",
       "      <td>0.817205</td>\n",
       "      <td>0.910616</td>\n",
       "      <td>13.685470</td>\n",
       "      <td>31.358980</td>\n",
       "      <td>16.804388</td>\n",
       "      <td>3.147368</td>\n",
       "      <td>-26.494103</td>\n",
       "      <td>-26.395575</td>\n",
       "      <td>-22.827836</td>\n",
       "      <td>24.446198</td>\n",
       "      <td>-26.430768</td>\n",
       "      <td>-26.347277</td>\n",
       "      <td>-26.410091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39607</th>\n",
       "      <td>TEST_39608</td>\n",
       "      <td>1.270898</td>\n",
       "      <td>0.924475</td>\n",
       "      <td>0.993878</td>\n",
       "      <td>13.796341</td>\n",
       "      <td>31.263376</td>\n",
       "      <td>16.705410</td>\n",
       "      <td>3.178268</td>\n",
       "      <td>-26.518929</td>\n",
       "      <td>-26.398439</td>\n",
       "      <td>-22.994543</td>\n",
       "      <td>24.358446</td>\n",
       "      <td>-26.451001</td>\n",
       "      <td>-26.386758</td>\n",
       "      <td>-26.392626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39608 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID      Y_01      Y_02      Y_03       Y_04       Y_05  \\\n",
       "0      TEST_00001  1.411203  1.270231  1.157351  12.729968  30.218679   \n",
       "1      TEST_00002  1.369738  1.225033  1.159070  13.507953  30.245127   \n",
       "2      TEST_00003  1.398354  1.085939  1.102530  15.891426  32.272181   \n",
       "3      TEST_00004  1.331356  1.033716  1.011200  14.956081  32.307154   \n",
       "4      TEST_00005  1.282550  1.038509  0.941753  14.411838  30.911137   \n",
       "...           ...       ...       ...       ...        ...        ...   \n",
       "39603  TEST_39604  1.281709  0.938325  1.007118  13.135560  30.885437   \n",
       "39604  TEST_39605  1.216538  0.909782  0.950816  14.245863  31.388069   \n",
       "39605  TEST_39606  1.249731  0.921726  0.935355  13.170882  31.174406   \n",
       "39606  TEST_39607  1.207937  0.817205  0.910616  13.685470  31.358980   \n",
       "39607  TEST_39608  1.270898  0.924475  0.993878  13.796341  31.263376   \n",
       "\n",
       "            Y_06      Y_07       Y_08       Y_09       Y_10       Y_11  \\\n",
       "0      15.317764  3.319986 -26.269786 -26.145384 -22.754328  24.207532   \n",
       "1      16.109916  3.252141 -26.293444 -26.494397 -22.507339  24.146070   \n",
       "2      16.139362  3.122304 -26.108558 -26.166355 -22.625140  24.391475   \n",
       "3      16.802362  3.272608 -25.862041 -25.904944 -21.993169  24.631787   \n",
       "4      16.733598  3.147980 -25.864387 -26.010221 -22.582241  24.451207   \n",
       "...          ...       ...        ...        ...        ...        ...   \n",
       "39603  16.648009  3.144713 -26.524722 -26.516356 -22.974088  24.275399   \n",
       "39604  16.732544  3.134051 -26.495966 -26.394527 -22.745000  24.349661   \n",
       "39605  16.668403  3.165195 -26.514139 -26.424853 -22.873743  24.359275   \n",
       "39606  16.804388  3.147368 -26.494103 -26.395575 -22.827836  24.446198   \n",
       "39607  16.705410  3.178268 -26.518929 -26.398439 -22.994543  24.358446   \n",
       "\n",
       "            Y_12       Y_13       Y_14  \n",
       "0     -26.150464 -26.176530 -26.158899  \n",
       "1     -26.266648 -26.331711 -26.237712  \n",
       "2     -26.045497 -26.080950 -26.145889  \n",
       "3     -25.865593 -25.891911 -25.745877  \n",
       "4     -25.655802 -25.848202 -25.847849  \n",
       "...          ...        ...        ...  \n",
       "39603 -26.429750 -26.435848 -26.414641  \n",
       "39604 -26.475413 -26.372099 -26.341783  \n",
       "39605 -26.422926 -26.412130 -26.434783  \n",
       "39606 -26.430768 -26.347277 -26.410091  \n",
       "39607 -26.451001 -26.386758 -26.392626  \n",
       "\n",
       "[39608 rows x 15 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(submit_pred)):\n",
    "    col = submit_df.columns[i+1]\n",
    "    submit_df[col] = np.mean(submit_pred[i],axis=0)\n",
    "    \n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYJhBI4-V3yA"
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"kalman_3stacking.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6XMqp4EV3yB",
    "outputId": "b33672d8-05e0-412a-ef37-25808797f5b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.411203\n",
       "1        1.369738\n",
       "2        1.398354\n",
       "3        1.331356\n",
       "4        1.282550\n",
       "           ...   \n",
       "39603    1.281709\n",
       "39604    1.216538\n",
       "39605    1.249731\n",
       "39606    1.207937\n",
       "39607    1.270898\n",
       "Name: Y_01, Length: 39608, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df['Y_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCHSPZLQV3yE",
    "outputId": "ac365985-8d02-484d-b644-ba51f4c0df48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_04_log' 'X_47' 'X_48' 'X_47_log' 'X_48_log' 'X_23' 'X_23_log' 'X_04'\n",
      " 'kf_X_45' 'kf_X_20' 'kf_X_3' 'kf_X_44' 'X_02_log' 'X_01_log' 'X_02'\n",
      " 'X_32_log' 'X_30_log' 'X_18' 'X_13_log' 'dif_X_14X_15' 'dif_X_17X_15'\n",
      " 'dif_X_15X_18' 'X_14_log' 'X_26_log' 'dif_X_32X_31' 'dif_X_16X_14'\n",
      " 'X_36_log' 'X_14' 'dif_X_16X_18' 'dif_X_18X_15' 'dif_X_17X_16'\n",
      " 'dif_X_16X_15' 'X_28_log' 'dif_X_15X_16' 'dif_X_30X_32' 'dif_X_33X_32'\n",
      " 'X_01' 'dif_X_18X_14' 'X_18_log' 'dif_X_32X_30' 'X_16_log' 'X_31_log'\n",
      " 'dif_X_14X_18' 'X_06_log' 'X_37_log' 'X_41_log' 'X_27_log' 'dif_X_15X_14'\n",
      " 'X_15_log' 'X_25_log' 'dif_X_17X_14' 'X_29_log' 'X_12_log' 'dif_X_14X_16'\n",
      " 'dif_X_33X_30' 'X_32' 'X_26' 'X_24_log' 'X_35_log' 'X_13' 'X_34_log'\n",
      " 'dif_X_21X_22' 'X_27' 'dif_X_30X_33' 'dif_X_31X_32' 'X_16' 'X_30'\n",
      " 'X_44_log' 'X_21_log' 'X_17' 'dif_X_16X_17' 'dif_X_15X_17' 'dif_X_30X_31'\n",
      " 'X_36' 'X_15' 'X_17_log' 'dif_X_31X_30' 'X_28' 'X_22_log' 'abs_X_42X_15'\n",
      " 'X_33_log' 'X_12' 'dif_X_14X_17' 'dif_X_32X_33' 'X_33' 'X_42_log' 'X_44'\n",
      " 'dif_X_18X_16' 'X_46' 'X_37' 'X_25' 'X_24' 'dif_X_31X_33' 'X_06'\n",
      " 'dif_X_20X_22' 'X_41' 'PCB_sum_1' 'X_19' 'X_20_log' 'X_31' 'dif_X_22X_20'\n",
      " 'dif_X_33X_31' 'X_20' 'dif_X_21X_19' 'X_29' 'dif_X_19X_20' 'X_46_log'\n",
      " 'X_54_log' 'X_35' 'X_55_log' 'dif_X_18X_17' 'dif_X_22X_21' 'X_08_log'\n",
      " 'X_22' 'X_43_log' 'X_34' 'X_19_log' 'X_38' 'X_40' 'X_50_log' 'X_56_log'\n",
      " 'dif_X_19X_21' 'X_21' 'dif_X_20X_19' 'dif_X_21X_20' 'X_55' 'kf_X_12'\n",
      " 'X_53_log' 'X_45' 'dif_X_20X_21' 'X_50' 'X_51' 'abs_X_41X_14' 'kf_X_25'\n",
      " 'kf_X_41' 'X_43' 'dif_X_17X_18' 'X_42' 'X_09_log' 'std_screw_depth'\n",
      " 'abs_X_44X_17' 'X_52_log' 'X_53' 'PCB_sum_2' 'X_03_log' 'X_51_log' 'X_39'\n",
      " 'abs_X_43X_16' 'kf_X_1' 'std_nthscrew_depth' 'std_connector' 'X_07_log'\n",
      " 'X_49_log' 'kf_X_29' 'kf_X_28' 'X_09' 'X_56' 'X_08' 'kf_X_26' 'X_52'\n",
      " 'kf_X_51' 'X_05' 'X_54' 'dif_X_22X_19' 'kf_X_48' 'kf_X_36' 'kf_X_18'\n",
      " 'kf_X_9' 'kf_X_5' 'var_antena_loc' 'kf_X_30' 'kf_X_23' 'kf_X_40'\n",
      " 'kf_X_37' 'kf_X_14' 'radi_sum_area' 'kf_X_33' 'X_05_log' 'kf_X_11'\n",
      " 'kf_X_38' 'kf_X_17' 'kf_X_39' 'kf_X_22' 'X_03' 'kf_X_34' 'X_07'\n",
      " 'dif_X_19X_22' 'kf_X_32' 'kf_X_10' 'kf_X_24' 'kf_X_0' 'kf_X_13' 'kf_X_53'\n",
      " 'kf_X_42' 'kf_X_50' 'kf_X_35' 'kf_X_27' 'kf_X_52' 'kf_X_47'\n",
      " 'radi_1_weight_per_area' 'kf_X_16' 'kf_X_49' 'kf_X_2' 'kf_X_31' 'kf_X_7'\n",
      " 'kf_X_15' 'kf_X_43' 'kf_X_4' 'kf_X_8' 'kf_X_21' 'kf_X_19' 'X_49' 'kf_X_6'\n",
      " 'kf_X_46']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "feature_importance = final_models[0][2].feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(submit_x.columns)[sorted_idx])\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "print(np.array(submit_x.columns)[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK_7kZcmV3yE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaT7IxYiV3yF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcVklcGYV3yF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6xtPSwYV3yF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WKFkZVQYVUS"
   },
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8j0H4yRLwByA"
   },
   "outputs": [],
   "source": [
    "model_idx=np.array(sorted(scores.items(), key = lambda item : item[1]))[:5, 0]\n",
    "best_ml_1=[value for i, value in enumerate(base_ml) if i in model_idx]\n",
    "best_ml_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHqIIftoV3yH"
   },
   "outputs": [],
   "source": [
    "best_ml_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5ktPjPfwB1F"
   },
   "outputs": [],
   "source": [
    "meta_clf=LinearRegression()\n",
    "meta_clf.fit(meta_ml_X_train, y_train)\n",
    "prediction=meta_clf.predict(meta_ml_X_test)\n",
    "\n",
    "## Valid\n",
    "\n",
    "nrmse_all=sep_lg_nrmse(y_test, prediction)\n",
    "print(\"nmae_all: \", nrmse_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp2-ZOAKV3yK",
    "outputId": "8da39d33-b66e-401e-cb58-d41f9313b81e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_ml_01' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24228/3833984127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_ml_01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_ml_02\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_ml_03\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_ml_04\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_ml_05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_ml_01' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_ml_01)\n",
    "print(best_ml_02)\n",
    "print(best_ml_03)\n",
    "print(best_ml_04)\n",
    "print(best_ml_05)\n",
    "print(best_ml_06)\n",
    "print(best_ml_07)\n",
    "print(best_ml_08)\n",
    "print(best_ml_09)\n",
    "print(best_ml_10)\n",
    "print(best_ml_11)\n",
    "print(best_ml_12)\n",
    "print(best_ml_13)\n",
    "print(best_ml_14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JICS8eH8V3yL"
   },
   "outputs": [],
   "source": [
    "meta_ml_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etxr0K1AV3yM"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in val_list:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_'+str(k)], test_size = 0.2, random_state=1422)\n",
    "    meta_ml_X_train=[]\n",
    "    meta_ml_X_test=[]\n",
    "    for estimator in tqdm(globals()['best_ml_%s' %k]):\n",
    "        temp_X_train, temp_X_test = get_stacking_ml_datasets(estimator, X_train, y_train, X_test, 5)\n",
    "        meta_ml_X_train.append(temp_X_train)\n",
    "        meta_ml_X_test.append(temp_X_test)\n",
    "\n",
    "    meta_ml_X_train=np.hstack(meta_ml_X_train)\n",
    "    meta_ml_X_test=np.hstack(meta_ml_X_test)\n",
    "\n",
    "    print(meta_ml_X_train.shape, meta_ml_X_test.shape)\n",
    "\n",
    "    globals()['meta_clf_%s' %k] = LinearRegression()\n",
    "    globals()['meta_clf_%s' %k].fit(meta_ml_X_train, y_train)\n",
    "    prediction=globals()['meta_clf_%s' %k].predict(meta_ml_X_test)\n",
    "    res.append(prediction)\n",
    "\n",
    "    nrmse_best= sep_lg_nrmse(y_test, prediction)\n",
    "    print(\"nrmse_best: \", nrmse_best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HkGRaXGV3yN"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lx7DaxFoV3yO"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axt0TI7FV3yP"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state=1422)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLgrbe7MV3yP"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(res).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQaJYz5bV3yQ"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,pd.DataFrame(res).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwWXMIEOV3yR"
   },
   "outputs": [],
   "source": [
    "def predict_model(k,submit, model, test_x):\n",
    "    res = model.predict(test_x)\n",
    "    submit['Y_'+k] = res\n",
    "    return submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqW0FGgtV3yS",
    "outputId": "8f824e24-a2b8-49ee-af03-e8c2319276d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Y_01</th>\n",
       "      <th>Y_02</th>\n",
       "      <th>Y_03</th>\n",
       "      <th>Y_04</th>\n",
       "      <th>Y_05</th>\n",
       "      <th>Y_06</th>\n",
       "      <th>Y_07</th>\n",
       "      <th>Y_08</th>\n",
       "      <th>Y_09</th>\n",
       "      <th>Y_10</th>\n",
       "      <th>Y_11</th>\n",
       "      <th>Y_12</th>\n",
       "      <th>Y_13</th>\n",
       "      <th>Y_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39603</th>\n",
       "      <td>TEST_39604</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39604</th>\n",
       "      <td>TEST_39605</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39605</th>\n",
       "      <td>TEST_39606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39606</th>\n",
       "      <td>TEST_39607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39607</th>\n",
       "      <td>TEST_39608</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39608 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  Y_01  Y_02  Y_03  Y_04  Y_05  Y_06  Y_07  Y_08  Y_09  Y_10  \\\n",
       "0      TEST_00001     0     0     0     0     0     0     0     0     0     0   \n",
       "1      TEST_00002     0     0     0     0     0     0     0     0     0     0   \n",
       "2      TEST_00003     0     0     0     0     0     0     0     0     0     0   \n",
       "3      TEST_00004     0     0     0     0     0     0     0     0     0     0   \n",
       "4      TEST_00005     0     0     0     0     0     0     0     0     0     0   \n",
       "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "39603  TEST_39604     0     0     0     0     0     0     0     0     0     0   \n",
       "39604  TEST_39605     0     0     0     0     0     0     0     0     0     0   \n",
       "39605  TEST_39606     0     0     0     0     0     0     0     0     0     0   \n",
       "39606  TEST_39607     0     0     0     0     0     0     0     0     0     0   \n",
       "39607  TEST_39608     0     0     0     0     0     0     0     0     0     0   \n",
       "\n",
       "       Y_11  Y_12  Y_13  Y_14  \n",
       "0         0     0     0     0  \n",
       "1         0     0     0     0  \n",
       "2         0     0     0     0  \n",
       "3         0     0     0     0  \n",
       "4         0     0     0     0  \n",
       "...     ...   ...   ...   ...  \n",
       "39603     0     0     0     0  \n",
       "39604     0     0     0     0  \n",
       "39605     0     0     0     0  \n",
       "39606     0     0     0     0  \n",
       "39607     0     0     0     0  \n",
       "\n",
       "[39608 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(base+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EhChtJwV3yT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-U9f0SOV3yU",
    "outputId": "99f80851-72be-4943-ab72-5a3d35a99e49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:39<00:00,  1.38it/s]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x[col+\"_log\"] = submit_x[col].apply(lambda x : math.log(x))\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+a+b] = submit_x[a]-submit_x[b]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['dif_'+b+a] = submit_x[b]-submit_x[a]\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['PCB_sum_1'] = submit_x['X_01']+submit_x['X_02']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['PCB_sum_2'] = submit_x['X_05']+submit_x['X_06']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:53: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['radi_sum_area'] = submit_x['X_07'] + submit_x['X_08']+ submit_x['X_09']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['radi_1_weight_per_area'] = submit_x['X_03'] / submit_x['X_07']\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_41X_14'] =abs(submit_x[\"X_41\"]- submit_x['X_14'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:56: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_42X_15'] =abs(submit_x[\"X_42\"]- submit_x['X_15'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_43X_16'] =abs(submit_x[\"X_43\"]- submit_x['X_16'])\n",
      "C:\\Users\\rkd20\\AppData\\Local\\Temp/ipykernel_30064/4256007863.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  submit_x['abs_X_44X_17'] =abs(submit_x[\"X_44\"]- submit_x['X_17'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFt3uUlQV3yW"
   },
   "outputs": [],
   "source": [
    "def get_stacking_preidct(model, X_train_n, y_train_n,y_test_n X_test_n, submit_x ,n_folds):\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    submit_pred = np.zeros((submit_x.shape[0], n_folds))\n",
    "    best_model = model\n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n, y_train_n)):\n",
    "        X_tr = X_train_n.iloc[train_index]\n",
    "        y_tr = y_train_n.iloc[train_index]\n",
    "        X_te = X_train_n.iloc[valid_index]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "        submit_pred[:, folder_counter] = model.predict(submit_x)\n",
    "        \n",
    "        nrmse_best= sep_lg_nrmse(y_test, val_prediction)\n",
    "        \n",
    "#     test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) \n",
    "#     submit_pred_mean = np.mean(submit_pred, axis=1).reshape(-1,1)    \n",
    "    \n",
    "    \n",
    "    return train_fold_pred, test_pred, submit_pred,best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5YqwqOAV3yY"
   },
   "outputs": [],
   "source": [
    "val_res = []\n",
    "submit_res = []\n",
    "all_meta_ml_X_train = []\n",
    "all_meta_ml_X_test=[]\n",
    "all_meta_ml_X_submit =[]\n",
    "for k in val_list:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_x, train_y['Y_'+str(k)], test_size = 0.2, random_state=1422)\n",
    "    meta_ml_X_train=[]\n",
    "    meta_ml_X_test=[]\n",
    "    meta_ml_X_submit = []\n",
    "    for estimator in tqdm(globals()['best_ml_%s' %k]):\n",
    "        temp_X_train, temp_X_test, temp_X_submit = get_stacking_preidct(estimator, X_train, y_train,y_test, X_test, submit_x, 5)\n",
    "        meta_ml_X_train.append(temp_X_train)\n",
    "        meta_ml_X_test.append(temp_X_test)\n",
    "        meta_ml_X_submit.append(temp_X_submit)\n",
    "\n",
    "    meta_ml_X_train=np.hstack(meta_ml_X_train)\n",
    "    meta_ml_X_test=np.hstack(meta_ml_X_test)\n",
    "    meta_ml_X_submit = np.hstack(meta_ml_X_submit)\n",
    "    all_meta_ml_X_train.append(meta_ml_X_train)\n",
    "    all_meta_ml_X_test.append(meta_ml_X_test)\n",
    "    all_meta_ml_X_submit.append(meta_ml_X_submit)\n",
    "\n",
    "#     print(meta_ml_X_train.shape, meta_ml_X_test.shape, meta_ml_X_submit.shape)\n",
    "\n",
    "#     globals()['meta_clf_%s' %k] = LinearRegression()\n",
    "#     globals()['meta_clf_%s' %k].fit(meta_ml_X_train, y_train)\n",
    "#     val_prediction=globals()['meta_clf_%s' %k].predict(meta_ml_X_test)\n",
    "#     res_prediction = globals()['meta_clf_%s' %k].predict(meta_ml_X_submit)\n",
    "#     val_res.append(val_prediction)\n",
    "#     submit_res.append(res_prediction)\n",
    "#     ## Valid\n",
    "\n",
    "#     nrmse_best= sep_lg_nrmse(y_test, val_prediction)\n",
    "#     print(\"nrmse_best: \", nrmse_best)\n",
    "#     submit_df['Y_'+k] = res_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDMAhUa_V3yZ"
   },
   "outputs": [],
   "source": [
    "all_meta_ml_X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7XVBFf1V3ya"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BJPURW8V3ya"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR5c9jRmV3yb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFLSR64nV3yb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy0JoixuV3yb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y78YlwPwV3yb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMXWVlsgV3yc"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.2, random_state=1422)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw1146FnV3yc"
   },
   "outputs": [],
   "source": [
    "lg_nrmse(y_test,pd.DataFrame(val_res).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v33SpvlDV3yd"
   },
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"top3_5fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vRdrRFwV3yd"
   },
   "outputs": [],
   "source": [
    "submit_res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ1Avl7BV3ye"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Aimers_best_beautified.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
